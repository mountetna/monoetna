window.pdocSearch = (function(){
/** elasticlunr - http://weixsong.github.io * Copyright (C) 2017 Oliver Nightingale * Copyright (C) 2017 Wei Song * MIT Licensed */!function(){function e(e){if(null===e||"object"!=typeof e)return e;var t=e.constructor();for(var n in e)e.hasOwnProperty(n)&&(t[n]=e[n]);return t}var t=function(e){var n=new t.Index;return n.pipeline.add(t.trimmer,t.stopWordFilter,t.stemmer),e&&e.call(n,n),n};t.version="0.9.5",lunr=t,t.utils={},t.utils.warn=function(e){return function(t){e.console&&console.warn&&console.warn(t)}}(this),t.utils.toString=function(e){return void 0===e||null===e?"":e.toString()},t.EventEmitter=function(){this.events={}},t.EventEmitter.prototype.addListener=function(){var e=Array.prototype.slice.call(arguments),t=e.pop(),n=e;if("function"!=typeof t)throw new TypeError("last argument must be a function");n.forEach(function(e){this.hasHandler(e)||(this.events[e]=[]),this.events[e].push(t)},this)},t.EventEmitter.prototype.removeListener=function(e,t){if(this.hasHandler(e)){var n=this.events[e].indexOf(t);-1!==n&&(this.events[e].splice(n,1),0==this.events[e].length&&delete this.events[e])}},t.EventEmitter.prototype.emit=function(e){if(this.hasHandler(e)){var t=Array.prototype.slice.call(arguments,1);this.events[e].forEach(function(e){e.apply(void 0,t)},this)}},t.EventEmitter.prototype.hasHandler=function(e){return e in this.events},t.tokenizer=function(e){if(!arguments.length||null===e||void 0===e)return[];if(Array.isArray(e)){var n=e.filter(function(e){return null===e||void 0===e?!1:!0});n=n.map(function(e){return t.utils.toString(e).toLowerCase()});var i=[];return n.forEach(function(e){var n=e.split(t.tokenizer.seperator);i=i.concat(n)},this),i}return e.toString().trim().toLowerCase().split(t.tokenizer.seperator)},t.tokenizer.defaultSeperator=/[\s\-]+/,t.tokenizer.seperator=t.tokenizer.defaultSeperator,t.tokenizer.setSeperator=function(e){null!==e&&void 0!==e&&"object"==typeof e&&(t.tokenizer.seperator=e)},t.tokenizer.resetSeperator=function(){t.tokenizer.seperator=t.tokenizer.defaultSeperator},t.tokenizer.getSeperator=function(){return t.tokenizer.seperator},t.Pipeline=function(){this._queue=[]},t.Pipeline.registeredFunctions={},t.Pipeline.registerFunction=function(e,n){n in t.Pipeline.registeredFunctions&&t.utils.warn("Overwriting existing registered function: "+n),e.label=n,t.Pipeline.registeredFunctions[n]=e},t.Pipeline.getRegisteredFunction=function(e){return e in t.Pipeline.registeredFunctions!=!0?null:t.Pipeline.registeredFunctions[e]},t.Pipeline.warnIfFunctionNotRegistered=function(e){var n=e.label&&e.label in this.registeredFunctions;n||t.utils.warn("Function is not registered with pipeline. This may cause problems when serialising the index.\n",e)},t.Pipeline.load=function(e){var n=new t.Pipeline;return e.forEach(function(e){var i=t.Pipeline.getRegisteredFunction(e);if(!i)throw new Error("Cannot load un-registered function: "+e);n.add(i)}),n},t.Pipeline.prototype.add=function(){var e=Array.prototype.slice.call(arguments);e.forEach(function(e){t.Pipeline.warnIfFunctionNotRegistered(e),this._queue.push(e)},this)},t.Pipeline.prototype.after=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i+1,0,n)},t.Pipeline.prototype.before=function(e,n){t.Pipeline.warnIfFunctionNotRegistered(n);var i=this._queue.indexOf(e);if(-1===i)throw new Error("Cannot find existingFn");this._queue.splice(i,0,n)},t.Pipeline.prototype.remove=function(e){var t=this._queue.indexOf(e);-1!==t&&this._queue.splice(t,1)},t.Pipeline.prototype.run=function(e){for(var t=[],n=e.length,i=this._queue.length,o=0;n>o;o++){for(var r=e[o],s=0;i>s&&(r=this._queue[s](r,o,e),void 0!==r&&null!==r);s++);void 0!==r&&null!==r&&t.push(r)}return t},t.Pipeline.prototype.reset=function(){this._queue=[]},t.Pipeline.prototype.get=function(){return this._queue},t.Pipeline.prototype.toJSON=function(){return this._queue.map(function(e){return t.Pipeline.warnIfFunctionNotRegistered(e),e.label})},t.Index=function(){this._fields=[],this._ref="id",this.pipeline=new t.Pipeline,this.documentStore=new t.DocumentStore,this.index={},this.eventEmitter=new t.EventEmitter,this._idfCache={},this.on("add","remove","update",function(){this._idfCache={}}.bind(this))},t.Index.prototype.on=function(){var e=Array.prototype.slice.call(arguments);return this.eventEmitter.addListener.apply(this.eventEmitter,e)},t.Index.prototype.off=function(e,t){return this.eventEmitter.removeListener(e,t)},t.Index.load=function(e){e.version!==t.version&&t.utils.warn("version mismatch: current "+t.version+" importing "+e.version);var n=new this;n._fields=e.fields,n._ref=e.ref,n.documentStore=t.DocumentStore.load(e.documentStore),n.pipeline=t.Pipeline.load(e.pipeline),n.index={};for(var i in e.index)n.index[i]=t.InvertedIndex.load(e.index[i]);return n},t.Index.prototype.addField=function(e){return this._fields.push(e),this.index[e]=new t.InvertedIndex,this},t.Index.prototype.setRef=function(e){return this._ref=e,this},t.Index.prototype.saveDocument=function(e){return this.documentStore=new t.DocumentStore(e),this},t.Index.prototype.addDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.addDoc(i,e),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));this.documentStore.addFieldLength(i,n,o.length);var r={};o.forEach(function(e){e in r?r[e]+=1:r[e]=1},this);for(var s in r){var u=r[s];u=Math.sqrt(u),this.index[n].addToken(s,{ref:i,tf:u})}},this),n&&this.eventEmitter.emit("add",e,this)}},t.Index.prototype.removeDocByRef=function(e){if(e&&this.documentStore.isDocStored()!==!1&&this.documentStore.hasDoc(e)){var t=this.documentStore.getDoc(e);this.removeDoc(t,!1)}},t.Index.prototype.removeDoc=function(e,n){if(e){var n=void 0===n?!0:n,i=e[this._ref];this.documentStore.hasDoc(i)&&(this.documentStore.removeDoc(i),this._fields.forEach(function(n){var o=this.pipeline.run(t.tokenizer(e[n]));o.forEach(function(e){this.index[n].removeToken(e,i)},this)},this),n&&this.eventEmitter.emit("remove",e,this))}},t.Index.prototype.updateDoc=function(e,t){var t=void 0===t?!0:t;this.removeDocByRef(e[this._ref],!1),this.addDoc(e,!1),t&&this.eventEmitter.emit("update",e,this)},t.Index.prototype.idf=function(e,t){var n="@"+t+"/"+e;if(Object.prototype.hasOwnProperty.call(this._idfCache,n))return this._idfCache[n];var i=this.index[t].getDocFreq(e),o=1+Math.log(this.documentStore.length/(i+1));return this._idfCache[n]=o,o},t.Index.prototype.getFields=function(){return this._fields.slice()},t.Index.prototype.search=function(e,n){if(!e)return[];e="string"==typeof e?{any:e}:JSON.parse(JSON.stringify(e));var i=null;null!=n&&(i=JSON.stringify(n));for(var o=new t.Configuration(i,this.getFields()).get(),r={},s=Object.keys(e),u=0;u<s.length;u++){var a=s[u];r[a]=this.pipeline.run(t.tokenizer(e[a]))}var l={};for(var c in o){var d=r[c]||r.any;if(d){var f=this.fieldSearch(d,c,o),h=o[c].boost;for(var p in f)f[p]=f[p]*h;for(var p in f)p in l?l[p]+=f[p]:l[p]=f[p]}}var v,g=[];for(var p in l)v={ref:p,score:l[p]},this.documentStore.hasDoc(p)&&(v.doc=this.documentStore.getDoc(p)),g.push(v);return g.sort(function(e,t){return t.score-e.score}),g},t.Index.prototype.fieldSearch=function(e,t,n){var i=n[t].bool,o=n[t].expand,r=n[t].boost,s=null,u={};return 0!==r?(e.forEach(function(e){var n=[e];1==o&&(n=this.index[t].expandToken(e));var r={};n.forEach(function(n){var o=this.index[t].getDocs(n),a=this.idf(n,t);if(s&&"AND"==i){var l={};for(var c in s)c in o&&(l[c]=o[c]);o=l}n==e&&this.fieldSearchStats(u,n,o);for(var c in o){var d=this.index[t].getTermFrequency(n,c),f=this.documentStore.getFieldLength(c,t),h=1;0!=f&&(h=1/Math.sqrt(f));var p=1;n!=e&&(p=.15*(1-(n.length-e.length)/n.length));var v=d*a*h*p;c in r?r[c]+=v:r[c]=v}},this),s=this.mergeScores(s,r,i)},this),s=this.coordNorm(s,u,e.length)):void 0},t.Index.prototype.mergeScores=function(e,t,n){if(!e)return t;if("AND"==n){var i={};for(var o in t)o in e&&(i[o]=e[o]+t[o]);return i}for(var o in t)o in e?e[o]+=t[o]:e[o]=t[o];return e},t.Index.prototype.fieldSearchStats=function(e,t,n){for(var i in n)i in e?e[i].push(t):e[i]=[t]},t.Index.prototype.coordNorm=function(e,t,n){for(var i in e)if(i in t){var o=t[i].length;e[i]=e[i]*o/n}return e},t.Index.prototype.toJSON=function(){var e={};return this._fields.forEach(function(t){e[t]=this.index[t].toJSON()},this),{version:t.version,fields:this._fields,ref:this._ref,documentStore:this.documentStore.toJSON(),index:e,pipeline:this.pipeline.toJSON()}},t.Index.prototype.use=function(e){var t=Array.prototype.slice.call(arguments,1);t.unshift(this),e.apply(this,t)},t.DocumentStore=function(e){this._save=null===e||void 0===e?!0:e,this.docs={},this.docInfo={},this.length=0},t.DocumentStore.load=function(e){var t=new this;return t.length=e.length,t.docs=e.docs,t.docInfo=e.docInfo,t._save=e.save,t},t.DocumentStore.prototype.isDocStored=function(){return this._save},t.DocumentStore.prototype.addDoc=function(t,n){this.hasDoc(t)||this.length++,this.docs[t]=this._save===!0?e(n):null},t.DocumentStore.prototype.getDoc=function(e){return this.hasDoc(e)===!1?null:this.docs[e]},t.DocumentStore.prototype.hasDoc=function(e){return e in this.docs},t.DocumentStore.prototype.removeDoc=function(e){this.hasDoc(e)&&(delete this.docs[e],delete this.docInfo[e],this.length--)},t.DocumentStore.prototype.addFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&(this.docInfo[e]||(this.docInfo[e]={}),this.docInfo[e][t]=n)},t.DocumentStore.prototype.updateFieldLength=function(e,t,n){null!==e&&void 0!==e&&0!=this.hasDoc(e)&&this.addFieldLength(e,t,n)},t.DocumentStore.prototype.getFieldLength=function(e,t){return null===e||void 0===e?0:e in this.docs&&t in this.docInfo[e]?this.docInfo[e][t]:0},t.DocumentStore.prototype.toJSON=function(){return{docs:this.docs,docInfo:this.docInfo,length:this.length,save:this._save}},t.stemmer=function(){var e={ational:"ate",tional:"tion",enci:"ence",anci:"ance",izer:"ize",bli:"ble",alli:"al",entli:"ent",eli:"e",ousli:"ous",ization:"ize",ation:"ate",ator:"ate",alism:"al",iveness:"ive",fulness:"ful",ousness:"ous",aliti:"al",iviti:"ive",biliti:"ble",logi:"log"},t={icate:"ic",ative:"",alize:"al",iciti:"ic",ical:"ic",ful:"",ness:""},n="[^aeiou]",i="[aeiouy]",o=n+"[^aeiouy]*",r=i+"[aeiou]*",s="^("+o+")?"+r+o,u="^("+o+")?"+r+o+"("+r+")?$",a="^("+o+")?"+r+o+r+o,l="^("+o+")?"+i,c=new RegExp(s),d=new RegExp(a),f=new RegExp(u),h=new RegExp(l),p=/^(.+?)(ss|i)es$/,v=/^(.+?)([^s])s$/,g=/^(.+?)eed$/,m=/^(.+?)(ed|ing)$/,y=/.$/,S=/(at|bl|iz)$/,x=new RegExp("([^aeiouylsz])\\1$"),w=new RegExp("^"+o+i+"[^aeiouwxy]$"),I=/^(.+?[^aeiou])y$/,b=/^(.+?)(ational|tional|enci|anci|izer|bli|alli|entli|eli|ousli|ization|ation|ator|alism|iveness|fulness|ousness|aliti|iviti|biliti|logi)$/,E=/^(.+?)(icate|ative|alize|iciti|ical|ful|ness)$/,D=/^(.+?)(al|ance|ence|er|ic|able|ible|ant|ement|ment|ent|ou|ism|ate|iti|ous|ive|ize)$/,F=/^(.+?)(s|t)(ion)$/,_=/^(.+?)e$/,P=/ll$/,k=new RegExp("^"+o+i+"[^aeiouwxy]$"),z=function(n){var i,o,r,s,u,a,l;if(n.length<3)return n;if(r=n.substr(0,1),"y"==r&&(n=r.toUpperCase()+n.substr(1)),s=p,u=v,s.test(n)?n=n.replace(s,"$1$2"):u.test(n)&&(n=n.replace(u,"$1$2")),s=g,u=m,s.test(n)){var z=s.exec(n);s=c,s.test(z[1])&&(s=y,n=n.replace(s,""))}else if(u.test(n)){var z=u.exec(n);i=z[1],u=h,u.test(i)&&(n=i,u=S,a=x,l=w,u.test(n)?n+="e":a.test(n)?(s=y,n=n.replace(s,"")):l.test(n)&&(n+="e"))}if(s=I,s.test(n)){var z=s.exec(n);i=z[1],n=i+"i"}if(s=b,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+e[o])}if(s=E,s.test(n)){var z=s.exec(n);i=z[1],o=z[2],s=c,s.test(i)&&(n=i+t[o])}if(s=D,u=F,s.test(n)){var z=s.exec(n);i=z[1],s=d,s.test(i)&&(n=i)}else if(u.test(n)){var z=u.exec(n);i=z[1]+z[2],u=d,u.test(i)&&(n=i)}if(s=_,s.test(n)){var z=s.exec(n);i=z[1],s=d,u=f,a=k,(s.test(i)||u.test(i)&&!a.test(i))&&(n=i)}return s=P,u=d,s.test(n)&&u.test(n)&&(s=y,n=n.replace(s,"")),"y"==r&&(n=r.toLowerCase()+n.substr(1)),n};return z}(),t.Pipeline.registerFunction(t.stemmer,"stemmer"),t.stopWordFilter=function(e){return e&&t.stopWordFilter.stopWords[e]!==!0?e:void 0},t.clearStopWords=function(){t.stopWordFilter.stopWords={}},t.addStopWords=function(e){null!=e&&Array.isArray(e)!==!1&&e.forEach(function(e){t.stopWordFilter.stopWords[e]=!0},this)},t.resetStopWords=function(){t.stopWordFilter.stopWords=t.defaultStopWords},t.defaultStopWords={"":!0,a:!0,able:!0,about:!0,across:!0,after:!0,all:!0,almost:!0,also:!0,am:!0,among:!0,an:!0,and:!0,any:!0,are:!0,as:!0,at:!0,be:!0,because:!0,been:!0,but:!0,by:!0,can:!0,cannot:!0,could:!0,dear:!0,did:!0,"do":!0,does:!0,either:!0,"else":!0,ever:!0,every:!0,"for":!0,from:!0,get:!0,got:!0,had:!0,has:!0,have:!0,he:!0,her:!0,hers:!0,him:!0,his:!0,how:!0,however:!0,i:!0,"if":!0,"in":!0,into:!0,is:!0,it:!0,its:!0,just:!0,least:!0,let:!0,like:!0,likely:!0,may:!0,me:!0,might:!0,most:!0,must:!0,my:!0,neither:!0,no:!0,nor:!0,not:!0,of:!0,off:!0,often:!0,on:!0,only:!0,or:!0,other:!0,our:!0,own:!0,rather:!0,said:!0,say:!0,says:!0,she:!0,should:!0,since:!0,so:!0,some:!0,than:!0,that:!0,the:!0,their:!0,them:!0,then:!0,there:!0,these:!0,they:!0,"this":!0,tis:!0,to:!0,too:!0,twas:!0,us:!0,wants:!0,was:!0,we:!0,were:!0,what:!0,when:!0,where:!0,which:!0,"while":!0,who:!0,whom:!0,why:!0,will:!0,"with":!0,would:!0,yet:!0,you:!0,your:!0},t.stopWordFilter.stopWords=t.defaultStopWords,t.Pipeline.registerFunction(t.stopWordFilter,"stopWordFilter"),t.trimmer=function(e){if(null===e||void 0===e)throw new Error("token should not be undefined");return e.replace(/^\W+/,"").replace(/\W+$/,"")},t.Pipeline.registerFunction(t.trimmer,"trimmer"),t.InvertedIndex=function(){this.root={docs:{},df:0}},t.InvertedIndex.load=function(e){var t=new this;return t.root=e.root,t},t.InvertedIndex.prototype.addToken=function(e,t,n){for(var n=n||this.root,i=0;i<=e.length-1;){var o=e[i];o in n||(n[o]={docs:{},df:0}),i+=1,n=n[o]}var r=t.ref;n.docs[r]?n.docs[r]={tf:t.tf}:(n.docs[r]={tf:t.tf},n.df+=1)},t.InvertedIndex.prototype.hasToken=function(e){if(!e)return!1;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return!1;t=t[e[n]]}return!0},t.InvertedIndex.prototype.getNode=function(e){if(!e)return null;for(var t=this.root,n=0;n<e.length;n++){if(!t[e[n]])return null;t=t[e[n]]}return t},t.InvertedIndex.prototype.getDocs=function(e){var t=this.getNode(e);return null==t?{}:t.docs},t.InvertedIndex.prototype.getTermFrequency=function(e,t){var n=this.getNode(e);return null==n?0:t in n.docs?n.docs[t].tf:0},t.InvertedIndex.prototype.getDocFreq=function(e){var t=this.getNode(e);return null==t?0:t.df},t.InvertedIndex.prototype.removeToken=function(e,t){if(e){var n=this.getNode(e);null!=n&&t in n.docs&&(delete n.docs[t],n.df-=1)}},t.InvertedIndex.prototype.expandToken=function(e,t,n){if(null==e||""==e)return[];var t=t||[];if(void 0==n&&(n=this.getNode(e),null==n))return t;n.df>0&&t.push(e);for(var i in n)"docs"!==i&&"df"!==i&&this.expandToken(e+i,t,n[i]);return t},t.InvertedIndex.prototype.toJSON=function(){return{root:this.root}},t.Configuration=function(e,n){var e=e||"";if(void 0==n||null==n)throw new Error("fields should not be null");this.config={};var i;try{i=JSON.parse(e),this.buildUserConfig(i,n)}catch(o){t.utils.warn("user configuration parse failed, will use default configuration"),this.buildDefaultConfig(n)}},t.Configuration.prototype.buildDefaultConfig=function(e){this.reset(),e.forEach(function(e){this.config[e]={boost:1,bool:"OR",expand:!1}},this)},t.Configuration.prototype.buildUserConfig=function(e,n){var i="OR",o=!1;if(this.reset(),"bool"in e&&(i=e.bool||i),"expand"in e&&(o=e.expand||o),"fields"in e)for(var r in e.fields)if(n.indexOf(r)>-1){var s=e.fields[r],u=o;void 0!=s.expand&&(u=s.expand),this.config[r]={boost:s.boost||0===s.boost?s.boost:1,bool:s.bool||i,expand:u}}else t.utils.warn("field name in user configuration not found in index instance fields");else this.addAllFields2UserConfig(i,o,n)},t.Configuration.prototype.addAllFields2UserConfig=function(e,t,n){n.forEach(function(n){this.config[n]={boost:1,bool:e,expand:t}},this)},t.Configuration.prototype.get=function(){return this.config},t.Configuration.prototype.reset=function(){this.config={}},lunr.SortedSet=function(){this.length=0,this.elements=[]},lunr.SortedSet.load=function(e){var t=new this;return t.elements=e,t.length=e.length,t},lunr.SortedSet.prototype.add=function(){var e,t;for(e=0;e<arguments.length;e++)t=arguments[e],~this.indexOf(t)||this.elements.splice(this.locationFor(t),0,t);this.length=this.elements.length},lunr.SortedSet.prototype.toArray=function(){return this.elements.slice()},lunr.SortedSet.prototype.map=function(e,t){return this.elements.map(e,t)},lunr.SortedSet.prototype.forEach=function(e,t){return this.elements.forEach(e,t)},lunr.SortedSet.prototype.indexOf=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;){if(r===e)return o;e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o]}return r===e?o:-1},lunr.SortedSet.prototype.locationFor=function(e){for(var t=0,n=this.elements.length,i=n-t,o=t+Math.floor(i/2),r=this.elements[o];i>1;)e>r&&(t=o),r>e&&(n=o),i=n-t,o=t+Math.floor(i/2),r=this.elements[o];return r>e?o:e>r?o+1:void 0},lunr.SortedSet.prototype.intersect=function(e){for(var t=new lunr.SortedSet,n=0,i=0,o=this.length,r=e.length,s=this.elements,u=e.elements;;){if(n>o-1||i>r-1)break;s[n]!==u[i]?s[n]<u[i]?n++:s[n]>u[i]&&i++:(t.add(s[n]),n++,i++)}return t},lunr.SortedSet.prototype.clone=function(){var e=new lunr.SortedSet;return e.elements=this.toArray(),e.length=e.elements.length,e},lunr.SortedSet.prototype.union=function(e){var t,n,i;this.length>=e.length?(t=this,n=e):(t=e,n=this),i=t.clone();for(var o=0,r=n.toArray();o<r.length;o++)i.add(r[o]);return i},lunr.SortedSet.prototype.toJSON=function(){return this.toArray()},function(e,t){"function"==typeof define&&define.amd?define(t):"object"==typeof exports?module.exports=t():e.elasticlunr=t()}(this,function(){return t})}();
    /** pdoc search index */const docs = [{"fullname": "etna", "modulename": "etna", "type": "module", "doc": "<p><code>etna</code> is a collection of airflow related utilities for interacting with\nand processing data inside the DSCOLabs Data Library project.</p>\n\n<p>These utilities are meant to supplement those provided directly by the <code>airflow</code>\npython library, whose basic interfaces and behaviors are compatible and composable\nwith all the utilities here within.  https://airflow.apache.org/docs/</p>\n\n<p>The top level module exports many common utilities needed by an average etl,\nwhile individual inner packages may contain implementations, interfaces, and\ngranular functions for nuanced use cases.</p>\n\n<p>This documentation is fairly exhaustive and likely too much for someone starting off.\nI highly recommend using this as a primary reference in combination with existing\nworking etl processes until a more comprehensive tutorial can be documented here.</p>\n"}, {"fullname": "etna.system_dag", "modulename": "etna", "qualname": "system_dag", "type": "function", "doc": "<p><code>dag</code> variant that sets the owner to 'administration' and a high number of retries.</p>\n", "signature": "(\n    interval: datetime.timedelta\n) -> Callable[[Callable], airflow.models.dag.DAG]", "funcdef": "def"}, {"fullname": "etna.dag", "modulename": "etna", "qualname": "dag", "type": "function", "doc": "<p>Simple convenience decorator that wraps airflow.dag decorator with some basic defaults and the ability\nto 'inject' parameters into the callable.  Generally not used directly, use other more specific decorators\nsuch as system_dag or metis_etl.</p>\n", "signature": "(\n    on_failure_callback: Union[Callable[[airflow.utils.context.Context], NoneType], NoneType] = None,\n    on_success_callback: Union[Callable[[airflow.utils.context.Context], NoneType], NoneType] = None,\n    schedule_interval: Union[str, datetime.timedelta, dateutil.relativedelta.relativedelta, NoneType, Type[airflow.models.dag.ScheduleIntervalArgNotSet]] = <class 'airflow.models.dag.ScheduleIntervalArgNotSet'>,\n    start_date: Union[datetime, NoneType] = None,\n    end_date: Union[datetime, NoneType] = None,\n    inject_params: Mapping[str, str] = {},\n    version: Union[int, str] = '',\n    **kwds\n)", "funcdef": "def"}, {"fullname": "etna.rollup_dag", "modulename": "etna", "qualname": "rollup_dag", "type": "function", "doc": "<p><code>system_dag</code> variant that sets the dag's labels so that a rollup of it's XCom values are used</p>\n", "signature": "(\n    interval: datetime.timedelta,\n    labels: List[str]\n) -> Callable[[Callable], airflow.models.dag.DAG]", "funcdef": "def"}, {"fullname": "etna.run_on_docker", "modulename": "etna", "qualname": "run_on_docker", "type": "function", "doc": "<p></p>\n", "signature": "(\n    task_id: str,\n    source_service: str,\n    command: List[str],\n    env: Mapping[str, str] = {},\n    output_json: Union[bool, Type] = False,\n    output_b64: bool = False,\n    docker_base_url='unix://var/run/docker.sock',\n    include_external_networks=False\n) -> etna.operators.docker_operator_base.DockerOperatorBase", "funcdef": "def"}, {"fullname": "etna.metis_etl", "modulename": "etna", "qualname": "metis_etl", "type": "function", "doc": "<p>A decorator that converts a decorated function into a DAG by the same name, using all tasks instantiated within.</p>\n\n<p>This decorator is the main entry point for processing metis files with an etl.  The decorated function receives\na number of useful XComArg objects and a helpers object to help set up metis tasks based on consuming changes\nscoped to the given project_name, bucket_name pair.</p>\n\n<ol>\n<li>tail_folders -- an XComArg object that will resolve into a list of Folder objects inside tasks and link functions.</li>\n<li>tail_files -- Similar to tail_folders, but with File objects.</li>\n<li>helpers -- a MetisEtlHelpers object that contains useful functions for common etl processing.</li>\n</ol>\n", "signature": "(\n    project_name: str,\n    bucket_name: str,\n    version: Union[int, str],\n    propagate_updates=True,\n    hook: Union[etna.hooks.etna.EtnaHook, NoneType] = None,\n    inject_params: Mapping[str, str] = {}\n)", "funcdef": "def"}, {"fullname": "etna.box_etl", "modulename": "etna", "qualname": "box_etl", "type": "function", "doc": "<p>A decorator that converts a decorated function into a DAG by the same name, using all tasks instantiated within.</p>\n\n<p>This decorator is the main entry point for processing Box files with an etl.  The decorated function receives\na number of useful XComArg objects and a helpers object to help set up Box tasks based on consuming changes\nscoped to the given folder in Box. The folder must be shared with the Etna service agent account.</p>\n\n<ol>\n<li>tail_files -- an XComArg object that will resolve into a list of File objects inside tasks.</li>\n</ol>\n", "signature": "(\n    folder_name: str,\n    version: Union[int, str],\n    hook: Union[etna.hooks.box.BoxHook, NoneType] = None,\n    project_name: str = 'administration',\n    inject_params: Mapping[str, str] = {}\n)", "funcdef": "def"}, {"fullname": "etna.link", "modulename": "etna", "qualname": "link", "type": "class", "doc": "<p></p>\n"}, {"fullname": "etna.link.__init__", "modulename": "etna", "qualname": "link.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    attribute_name: Union[str, NoneType] = None,\n    dry_run=True,\n    hook: Union[etna.hooks.etna.EtnaHook, NoneType] = None,\n    task_id: Union[str, NoneType] = None,\n    validate_record_update: Union[Callable[[Any, Any, List[str]], bool], NoneType] = None,\n    batch_size=30\n)", "funcdef": "def"}, {"fullname": "etna.link.project_name", "modulename": "etna", "qualname": "link.project_name", "type": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "etna.link.hook", "modulename": "etna", "qualname": "link.hook", "type": "variable", "doc": "<p></p>\n"}, {"fullname": "etna.MetisEtlHelpers", "modulename": "etna", "qualname": "MetisEtlHelpers", "type": "class", "doc": "<p></p>\n"}, {"fullname": "etna.MetisEtlHelpers.__init__", "modulename": "etna", "qualname": "MetisEtlHelpers.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    tail_folders: airflow.models.xcom_arg.XComArg,\n    tail_files: airflow.models.xcom_arg.XComArg,\n    hook: etna.hooks.etna.EtnaHook\n)", "funcdef": "def"}, {"fullname": "etna.MetisEtlHelpers.prepare_task_token", "modulename": "etna", "qualname": "MetisEtlHelpers.prepare_task_token", "type": "function", "doc": "<p>Returns an XComArg that can be used to pass a task token into other tasks.\nGenerally, this is used with <code>run_on_docker</code> to pass a task token into an input\nfile.</p>\n", "signature": "(self, read_only: bool) -> airflow.models.xcom_arg.XComArg", "funcdef": "def"}, {"fullname": "etna.MetisEtlHelpers.process_and_link_matching_file", "modulename": "etna", "qualname": "MetisEtlHelpers.process_and_link_matching_file", "type": "function", "doc": "<p>Creates a task that follows the listed set of record match folders and calls the given\nprocessor function to provide an UpdateRequest object as a result of matching files from\nthe listed match record folder.</p>\n\n<p>The decorated function can receive arguments named metis, file, match, or template,\neach being either a MetisClient, File, MatchedRecordFolder, or Template respectively.</p>\n\n<p>eg:</p>\n\n<pre><code>def my_processor(metis, template, match, file):\n  with metis.open_file(file) as file_reader:\n    csv_reader = csv.reader(file_reader)\n    return match.as_update(csv_reader)\n\nmatches = helpers.find_record_folders('rna_seq', rna_seq_folder_regex)\nlisted_matches = helpers.list_match_folders(matches)\nhelpers.process_and_link_matching_file(listed_matches, re.compile(r'.*gene_counts\\.tsv$'), my_processor)\n</code></pre>\n", "signature": "(\n    self,\n    listed_record_matches: airflow.models.xcom_arg.XComArg,\n    regex: re.Pattern,\n    processor: Callable[[], etna.hooks.etna.UpdateRequest],\n    dry_run=True\n) -> airflow.models.xcom_arg.XComArg", "funcdef": "def"}, {"fullname": "etna.MetisEtlHelpers.link_single_cell_attribute_files_v1", "modulename": "etna", "qualname": "MetisEtlHelpers.link_single_cell_attribute_files_v1", "type": "function", "doc": "<p>Links single cell rna related attributes for either sc_rna_seq or sc_rna_seq_pool models, including\nraw fastqs and processed files.</p>\n\n<p>Checkout <code>etna.etls.linkers.signle_cell_rna_seq.link_single_cell_attribute_files_v1</code> source code\nfor more information on how matches are made and which attributes are linked.</p>\n", "signature": "(\n    self,\n    model_name: str,\n    identifier_prefix: str,\n    single_cell_root_prefix='single_cell_',\n    attribute_linker_overrides: Union[Mapping[str, str], NoneType] = None,\n    link_parents: Union[Callable[[List[etna.etls.metis.MatchedRecordFolder]], Iterable[Tuple[etna.etls.metis.MatchedRecordFolder, etna.hooks.etna.UpdateRequest]]], NoneType] = None,\n    dry_run=True\n) -> Tuple[airflow.models.xcom_arg.XComArg, airflow.models.xcom_arg.XComArg]", "funcdef": "def"}, {"fullname": "etna.MetisEtlHelpers.link_matching_file", "modulename": "etna", "qualname": "MetisEtlHelpers.link_matching_file", "type": "function", "doc": "<p>Creates a task that iterates the provided listed_record_matches (the result of a helpers.list_match_folders\ncall), searching for files whose file_name matches the given regex, linking the file path to the given\nmatch's record in magma at the given attr_name attribute.</p>\n\n<p>Notably, when multiple files are found, the -last- file detected will be used in magma.  If you intend to link\nseveral matching files, use link_matching_files for attributes that are file_collection.</p>\n", "signature": "(\n    self,\n    listed_record_matches: airflow.models.xcom_arg.XComArg,\n    attr_name: str,\n    regex: re.Pattern,\n    dry_run=True\n) -> airflow.models.xcom_arg.XComArg", "funcdef": "def"}, {"fullname": "etna.MetisEtlHelpers.link_matching_files", "modulename": "etna", "qualname": "MetisEtlHelpers.link_matching_files", "type": "function", "doc": "<p>Like link_matching_file, except for two key points:</p>\n\n<ol>\n<li>all matches found are linked together as a file collection</li>\n<li>An additional argument, folder_path_regex, is matched against the folder before linking.\nBecause listed_record_matches ONLY sees a single directory's contents at a time, this\nfolder_path_regex ensures the correct folder is selected before linking, in case multiple\nchild directories exist for a matched record folder.</li>\n</ol>\n", "signature": "(\n    self,\n    listed_record_matches: airflow.models.xcom_arg.XComArg,\n    attr_name: str,\n    file_regex: re.Pattern,\n    folder_path_regex: re.Pattern = re.compile('^$'),\n    dry_run=True\n) -> airflow.models.xcom_arg.XComArg", "funcdef": "def"}, {"fullname": "etna.MetisEtlHelpers.link_record_folders_to_parent", "modulename": "etna", "qualname": "MetisEtlHelpers.link_record_folders_to_parent", "type": "function", "doc": "<p>When record folders are embedded in metis such that parent model record folders\nexist, it is common to want to 'link' the parent model attribute based on the folder name.</p>\n\n<p>This function takes an XComArg containing a list of MatchedRecordFolder obtained via\n<code>find_record_folders</code>, <em>whose source arg was not None</em>.  In other words, consider this example:</p>\n\n<p>Imagine a directory structure like /MYSAMPLE.T1/ScRnaSeq/MYSAMPLE.T1.BLAH.BLAH</p>\n\n<pre><code>sample_matches = helpers.find_record_folders(\"sample\", SAMPLE_REGEX)\nwith TaskGroup(\"sc_rna_seq_matches\"):\n  sc_rna_seq_matches = helpers.find_record_folders(\"sc_rna_seq\", SC_RNA_SEQ_REGEX, source=sample_matches)\n  helpers.link_record_folders_to_parent(sc_rna_esq_matches)\n</code></pre>\n\n<p>In this case, FIRST task will find \"/MYSAMPLE.T1\" as a \"sample\" record folder and it will belong\nto the <code>XComArg</code> returned as <code>sample_matches</code>.</p>\n\n<p>Next, it will search under /MYSAMPLE.T1 to find the MYSAMPLE.T1.BLAH.BLAH directory as a \"sc_rna_eq\" model\ndirectory, noting the parent record folder as MYSAMPLE.T1 and that pairing will belong to sc_rna_seq_matches.</p>\n\n<p>Lastly, the <code>link_record_folders_to_parent</code> will use the association between MYSAMPLE.T1.BLAH.BLAH and MYSAMPLE.T1\nto make the association between that sc_rna_seq record and the parent sample record.</p>\n", "signature": "(\n    self,\n    embedded_matches: airflow.models.xcom_arg.XComArg,\n    dry_run=True\n)", "funcdef": "def"}, {"fullname": "etna.MetisEtlHelpers.find_record_folders", "modulename": "etna", "qualname": "MetisEtlHelpers.find_record_folders", "type": "function", "doc": "<p>Creates a task that Searches for folders that match the given a regex creates MatchedRecordFolder\nentries with the name of the directory at the far end of the match and the given model name.  The match must\noccur at a folder boundary, so for instance, r'abc/def' does not match 'abc/defg' but does match 'abc/def/c'.\nThe directory matched by the last path segment of the regex is considered a 'record name' belonging to the given\nmodel_name, allowing other convenience methods to process and filter with that assumption.</p>\n\n<p><code>source</code> should be used when processing inner record folders, with the outer record matches XComArg being passed.\nIn that case, you should use a <code>with TaskGroup('---')</code> surrounding this task to distinguish the inner task name\nfrom the outer one.</p>\n", "signature": "(\n    self,\n    model_name: str,\n    regex: re.Pattern,\n    corrected_record_name: Callable[[str], str] = <function MetisEtlHelpers.<lambda>>,\n    source: Union[airflow.models.xcom_arg.XComArg, NoneType] = None\n) -> airflow.models.xcom_arg.XComArg", "funcdef": "def"}, {"fullname": "etna.MetisEtlHelpers.filter_by_timur", "modulename": "etna", "qualname": "MetisEtlHelpers.filter_by_timur", "type": "function", "doc": "<p>Creates a task that first validates the model / record name of the provided record folder matches already exist\nin magma / timur, ensuring that any incorrectly named folder does not accidentally become a magma record.</p>\n", "signature": "(\n    self,\n    matches: airflow.models.xcom_arg.XComArg\n) -> airflow.models.xcom_arg.XComArg", "funcdef": "def"}, {"fullname": "etna.MetisEtlHelpers.list_match_folders", "modulename": "etna", "qualname": "MetisEtlHelpers.list_match_folders", "type": "function", "doc": "<p>Creates a task that lists the contents of the given MatchedRecordFolders.  For instance, if a single file\nis added to a record folder, the entire contents of that record folder are listed for use by downstream\nlinkers.  this is important when linking multiple files in a collection, or processing that involves\nreading in multiple files that exist in the same record folder.</p>\n\n<p>The resulting XComArg will provide a value to any consuming task a value of List[Tuple[MatchedRecordFolder, List[File]]]</p>\n", "signature": "(self, matches: airflow.models.xcom_arg.XComArg)", "funcdef": "def"}, {"fullname": "etna.BoxEtlHelpers", "modulename": "etna", "qualname": "BoxEtlHelpers", "type": "class", "doc": "<p></p>\n"}, {"fullname": "etna.BoxEtlHelpers.__init__", "modulename": "etna", "qualname": "BoxEtlHelpers.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, hook: etna.hooks.box.BoxHook, box_folder: str)", "funcdef": "def"}, {"fullname": "etna.BoxEtlHelpers.alert_slack", "modulename": "etna", "qualname": "BoxEtlHelpers.alert_slack", "type": "function", "doc": "<p>Sends a Slack message to the data-ingest-ping channel, notifying of\n    the number of files uploaded.</p>\n\n<p>args:\n    files: List of files\n    ingested: bool, not really used, just helps control the flow of when messages are sent. Should be return value of helpers.ingest_to_metis.\n    project_name: str, project name for the message\n    bucket_name: str, bucket name for the message</p>\n", "signature": "(\n    self,\n    files: airflow.models.xcom_arg.XComArg,\n    ingested: bool,\n    project_name: str,\n    bucket_name: str\n)", "funcdef": "def"}, {"fullname": "etna.BoxEtlHelpers.filter_files", "modulename": "etna", "qualname": "BoxEtlHelpers.filter_files", "type": "function", "doc": "<p>Creates a task that filters the Box file names by regexp, and can also apply a regexp against the\nfolder path for each file for further filtering.</p>\n\n<p>args:\n    files: List of files from tail_files or previous filter_files call\n    file_name_regex: re.Pattern, i.e. re.compile(r\".<em>.fcs\")\n    folder_path_regex: re.Pattern, i.e. re.compile(r\".</em>ipi.*\")</p>\n", "signature": "(\n    self,\n    files: airflow.models.xcom_arg.XComArg,\n    file_name_regex: re.Pattern = re.compile('.*'),\n    folder_path_regex: re.Pattern = re.compile('.*')\n)", "funcdef": "def"}, {"fullname": "etna.BoxEtlHelpers.ingest_to_metis", "modulename": "etna", "qualname": "BoxEtlHelpers.ingest_to_metis", "type": "function", "doc": "<p>Given a list of Box files, will copy them to the given Metis project_name and bucket_name,\nmimicking the full directory structure from Box.</p>\n\n<p>args:\n    files: List of files from tail_files or filter_files call\n    project_name: str, the target Metis project name. Default is <code>triage</code>\n    bucket_name: str, the target Metis bucket name. Default is <code>waiting_room</code>\n    folder_path: str, existing folder path to dump the files in. Default is Box hostname + folder structure in Box.\n    flatten: bool, to flatten the Box folder structure or maintain it. Default is True.\n    clean_up: bool, to remove the file from Box after ingest. Default is False.\n    split_folder_name: str, if flatten=False, the folder name after which to copy the structure from Box. Generally would match what you use in filter_files() for folder_path_regex.</p>\n", "signature": "(\n    self,\n    files: airflow.models.xcom_arg.XComArg,\n    project_name: str = 'triage',\n    bucket_name: str = 'waiting_room',\n    folder_path: str = None,\n    flatten: bool = True,\n    clean_up: bool = False,\n    split_folder_name: str = None\n) -> airflow.models.xcom_arg.XComArg", "funcdef": "def"}, {"fullname": "etna.pickled", "modulename": "etna", "qualname": "pickled", "type": "function", "doc": "<p></p>\n", "signature": "(v: ~T) -> ~T", "funcdef": "def"}, {"fullname": "etna.get_project_name", "modulename": "etna", "qualname": "get_project_name", "type": "function", "doc": "<p>Utility function that retrieves the current project context that a task may be executing in.\nNote that this function may only be called _inside of an executing task_, such as a @link decorated\nfunction or a @task decorated function.  A DAG's project is determined by the first owner entry, which\nis set by most utilities in the etna library automatically.</p>\n", "signature": "(dag: Union[airflow.models.dag.DAG, NoneType] = None)", "funcdef": "def"}, {"fullname": "etna.get_etna_hook", "modulename": "etna", "qualname": "get_etna_hook", "type": "function", "doc": "<p></p>\n", "signature": "() -> etna.hooks.etna.EtnaHook", "funcdef": "def"}, {"fullname": "etna.get_git_hook", "modulename": "etna", "qualname": "get_git_hook", "type": "function", "doc": "<p></p>\n", "signature": "(remote_path: str) -> etna.hooks.git.GitHook", "funcdef": "def"}, {"fullname": "etna.get_project_slack_hook", "modulename": "etna", "qualname": "get_project_slack_hook", "type": "function", "doc": "<p></p>\n", "signature": "() -> airflow.providers.slack.hooks.slack.SlackHook", "funcdef": "def"}, {"fullname": "etna.UpdateRequest", "modulename": "etna", "qualname": "UpdateRequest", "type": "class", "doc": "<p>UpdateRequest(revisions: Dict[str, Dict[str, Dict[str, Any]]] = <factory>, project_name: str = '', dry_run: bool = False)</p>\n"}, {"fullname": "etna.UpdateRequest.__init__", "modulename": "etna", "qualname": "UpdateRequest.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    revisions: Dict[str, Dict[str, Dict[str, Any]]] = <factory>,\n    project_name: str = '',\n    dry_run: bool = False\n)", "funcdef": "def"}, {"fullname": "etna.UpdateRequest.project_name", "modulename": "etna", "qualname": "UpdateRequest.project_name", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.UpdateRequest.dry_run", "modulename": "etna", "qualname": "UpdateRequest.dry_run", "type": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": " = False"}, {"fullname": "etna.UpdateRequest.shallow_copy", "modulename": "etna", "qualname": "UpdateRequest.shallow_copy", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    revisions: Union[Dict[str, Dict[str, Dict[str, Any]]], NoneType] = None\n) -> etna.hooks.etna.UpdateRequest", "funcdef": "def"}, {"fullname": "etna.UpdateRequest.includes", "modulename": "etna", "qualname": "UpdateRequest.includes", "type": "function", "doc": "<p></p>\n", "signature": "(self, model_name: str, record_name: str)", "funcdef": "def"}, {"fullname": "etna.UpdateRequest.sample_revision_tree", "modulename": "etna", "qualname": "UpdateRequest.sample_revision_tree", "type": "function", "doc": "<p>Attempts to sample the complete graph of related objects reference by the given model_name / record_name in this\nupdate, copying it and all its referenced links to acc.</p>\n\n<p>When destructive=True is set, it also removes all copied models from this update.</p>\n", "signature": "(\n    self,\n    model_name: str,\n    record_name: str,\n    models: Dict[str, etna.hooks.etna.Model],\n    acc: etna.hooks.etna.UpdateRequest,\n    destructive=False\n)", "funcdef": "def"}, {"fullname": "etna.UpdateRequest.empty", "modulename": "etna", "qualname": "UpdateRequest.empty", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "etna.UpdateRequest.extend", "modulename": "etna", "qualname": "UpdateRequest.extend", "type": "function", "doc": "<p></p>\n", "signature": "(self, other: etna.hooks.etna.UpdateRequest)", "funcdef": "def"}, {"fullname": "etna.UpdateRequest.validate", "modulename": "etna", "qualname": "UpdateRequest.validate", "type": "function", "doc": "<p></p>\n", "signature": "(self, models: Dict[str, etna.hooks.etna.Model]) -> Iterable[str]", "funcdef": "def"}, {"fullname": "etna.UpdateRequest.update_record", "modulename": "etna", "qualname": "UpdateRequest.update_record", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    model_name: str,\n    record_name: str,\n    attrs: Dict[str, Any] = {}\n) -> Dict[str, Any]", "funcdef": "def"}, {"fullname": "etna.UpdateRequest.append_table", "modulename": "etna", "qualname": "UpdateRequest.append_table", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    parent_model_name: str,\n    parent_record_name: str,\n    model_name: str,\n    attrs: Dict[str, Any],\n    attribute_name: str\n)", "funcdef": "def"}, {"fullname": "etna.rollup", "modulename": "etna", "qualname": "rollup", "type": "class", "doc": "<p></p>\n"}, {"fullname": "etna.rollup.__init__", "modulename": "etna", "qualname": "rollup.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    concat: Callable[[etna.metrics.rollup_metrics.Rollup, etna.metrics.rollup_metrics.Rollup], etna.metrics.rollup_metrics.Rollup]\n)", "funcdef": "def"}, {"fullname": "etna.dags", "modulename": "etna.dags", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.dags.callbacks", "modulename": "etna.dags.callbacks", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.dags.callbacks.notify_slack_dag_callback", "modulename": "etna.dags.callbacks", "qualname": "notify_slack_dag_callback", "type": "function", "doc": "<p></p>\n", "signature": "(\n    dag_status: str\n) -> Callable[[airflow.utils.context.Context], NoneType]", "funcdef": "def"}, {"fullname": "etna.dags.decorators", "modulename": "etna.dags.decorators", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.dags.decorators.dag", "modulename": "etna.dags.decorators", "qualname": "dag", "type": "function", "doc": "<p>Simple convenience decorator that wraps airflow.dag decorator with some basic defaults and the ability\nto 'inject' parameters into the callable.  Generally not used directly, use other more specific decorators\nsuch as system_dag or metis_etl.</p>\n", "signature": "(\n    on_failure_callback: Union[Callable[[airflow.utils.context.Context], NoneType], NoneType] = None,\n    on_success_callback: Union[Callable[[airflow.utils.context.Context], NoneType], NoneType] = None,\n    schedule_interval: Union[str, datetime.timedelta, dateutil.relativedelta.relativedelta, NoneType, Type[airflow.models.dag.ScheduleIntervalArgNotSet]] = <class 'airflow.models.dag.ScheduleIntervalArgNotSet'>,\n    start_date: Union[datetime, NoneType] = None,\n    end_date: Union[datetime, NoneType] = None,\n    inject_params: Mapping[str, str] = {},\n    version: Union[int, str] = '',\n    **kwds\n)", "funcdef": "def"}, {"fullname": "etna.dags.decorators.rollup_dag", "modulename": "etna.dags.decorators", "qualname": "rollup_dag", "type": "function", "doc": "<p><code>system_dag</code> variant that sets the dag's labels so that a rollup of it's XCom values are used</p>\n", "signature": "(\n    interval: datetime.timedelta,\n    labels: List[str]\n) -> Callable[[Callable], airflow.models.dag.DAG]", "funcdef": "def"}, {"fullname": "etna.dags.decorators.system_dag", "modulename": "etna.dags.decorators", "qualname": "system_dag", "type": "function", "doc": "<p><code>dag</code> variant that sets the owner to 'administration' and a high number of retries.</p>\n", "signature": "(\n    interval: datetime.timedelta\n) -> Callable[[Callable], airflow.models.dag.DAG]", "funcdef": "def"}, {"fullname": "etna.dags.project_name", "modulename": "etna.dags.project_name", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.dags.project_name.get_project_name", "modulename": "etna.dags.project_name", "qualname": "get_project_name", "type": "function", "doc": "<p>Utility function that retrieves the current project context that a task may be executing in.\nNote that this function may only be called _inside of an executing task_, such as a @link decorated\nfunction or a @task decorated function.  A DAG's project is determined by the first owner entry, which\nis set by most utilities in the etna library automatically.</p>\n", "signature": "(dag: Union[airflow.models.dag.DAG, NoneType] = None)", "funcdef": "def"}, {"fullname": "etna.etls", "modulename": "etna.etls", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.etls.box", "modulename": "etna.etls.box", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.etls.box.BoxEtlHelpers", "modulename": "etna.etls.box", "qualname": "BoxEtlHelpers", "type": "class", "doc": "<p></p>\n"}, {"fullname": "etna.etls.box.BoxEtlHelpers.__init__", "modulename": "etna.etls.box", "qualname": "BoxEtlHelpers.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, hook: etna.hooks.box.BoxHook, box_folder: str)", "funcdef": "def"}, {"fullname": "etna.etls.box.BoxEtlHelpers.alert_slack", "modulename": "etna.etls.box", "qualname": "BoxEtlHelpers.alert_slack", "type": "function", "doc": "<p>Sends a Slack message to the data-ingest-ping channel, notifying of\n    the number of files uploaded.</p>\n\n<p>args:\n    files: List of files\n    ingested: bool, not really used, just helps control the flow of when messages are sent. Should be return value of helpers.ingest_to_metis.\n    project_name: str, project name for the message\n    bucket_name: str, bucket name for the message</p>\n", "signature": "(\n    self,\n    files: airflow.models.xcom_arg.XComArg,\n    ingested: bool,\n    project_name: str,\n    bucket_name: str\n)", "funcdef": "def"}, {"fullname": "etna.etls.box.BoxEtlHelpers.filter_files", "modulename": "etna.etls.box", "qualname": "BoxEtlHelpers.filter_files", "type": "function", "doc": "<p>Creates a task that filters the Box file names by regexp, and can also apply a regexp against the\nfolder path for each file for further filtering.</p>\n\n<p>args:\n    files: List of files from tail_files or previous filter_files call\n    file_name_regex: re.Pattern, i.e. re.compile(r\".<em>.fcs\")\n    folder_path_regex: re.Pattern, i.e. re.compile(r\".</em>ipi.*\")</p>\n", "signature": "(\n    self,\n    files: airflow.models.xcom_arg.XComArg,\n    file_name_regex: re.Pattern = re.compile('.*'),\n    folder_path_regex: re.Pattern = re.compile('.*')\n)", "funcdef": "def"}, {"fullname": "etna.etls.box.BoxEtlHelpers.ingest_to_metis", "modulename": "etna.etls.box", "qualname": "BoxEtlHelpers.ingest_to_metis", "type": "function", "doc": "<p>Given a list of Box files, will copy them to the given Metis project_name and bucket_name,\nmimicking the full directory structure from Box.</p>\n\n<p>args:\n    files: List of files from tail_files or filter_files call\n    project_name: str, the target Metis project name. Default is <code>triage</code>\n    bucket_name: str, the target Metis bucket name. Default is <code>waiting_room</code>\n    folder_path: str, existing folder path to dump the files in. Default is Box hostname + folder structure in Box.\n    flatten: bool, to flatten the Box folder structure or maintain it. Default is True.\n    clean_up: bool, to remove the file from Box after ingest. Default is False.\n    split_folder_name: str, if flatten=False, the folder name after which to copy the structure from Box. Generally would match what you use in filter_files() for folder_path_regex.</p>\n", "signature": "(\n    self,\n    files: airflow.models.xcom_arg.XComArg,\n    project_name: str = 'triage',\n    bucket_name: str = 'waiting_room',\n    folder_path: str = None,\n    flatten: bool = True,\n    clean_up: bool = False,\n    split_folder_name: str = None\n) -> airflow.models.xcom_arg.XComArg", "funcdef": "def"}, {"fullname": "etna.etls.box.load_box_files_batch", "modulename": "etna.etls.box", "qualname": "load_box_files_batch", "type": "function", "doc": "<p></p>\n", "signature": "(\n    box: etna.hooks.box.Box,\n    folder_name: str\n) -> List[etna.hooks.box.FtpEntry]", "funcdef": "def"}, {"fullname": "etna.etls.decorators", "modulename": "etna.etls.decorators", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.etls.decorators.metis_etl", "modulename": "etna.etls.decorators", "qualname": "metis_etl", "type": "function", "doc": "<p>A decorator that converts a decorated function into a DAG by the same name, using all tasks instantiated within.</p>\n\n<p>This decorator is the main entry point for processing metis files with an etl.  The decorated function receives\na number of useful XComArg objects and a helpers object to help set up metis tasks based on consuming changes\nscoped to the given project_name, bucket_name pair.</p>\n\n<ol>\n<li>tail_folders -- an XComArg object that will resolve into a list of Folder objects inside tasks and link functions.</li>\n<li>tail_files -- Similar to tail_folders, but with File objects.</li>\n<li>helpers -- a MetisEtlHelpers object that contains useful functions for common etl processing.</li>\n</ol>\n", "signature": "(\n    project_name: str,\n    bucket_name: str,\n    version: Union[int, str],\n    propagate_updates=True,\n    hook: Union[etna.hooks.etna.EtnaHook, NoneType] = None,\n    inject_params: Mapping[str, str] = {}\n)", "funcdef": "def"}, {"fullname": "etna.etls.decorators.box_etl", "modulename": "etna.etls.decorators", "qualname": "box_etl", "type": "function", "doc": "<p>A decorator that converts a decorated function into a DAG by the same name, using all tasks instantiated within.</p>\n\n<p>This decorator is the main entry point for processing Box files with an etl.  The decorated function receives\na number of useful XComArg objects and a helpers object to help set up Box tasks based on consuming changes\nscoped to the given folder in Box. The folder must be shared with the Etna service agent account.</p>\n\n<ol>\n<li>tail_files -- an XComArg object that will resolve into a list of File objects inside tasks.</li>\n</ol>\n", "signature": "(\n    folder_name: str,\n    version: Union[int, str],\n    hook: Union[etna.hooks.box.BoxHook, NoneType] = None,\n    project_name: str = 'administration',\n    inject_params: Mapping[str, str] = {}\n)", "funcdef": "def"}, {"fullname": "etna.etls.decorators.etl", "modulename": "etna.etls.decorators", "qualname": "etl", "type": "function", "doc": "<p></p>\n", "signature": "(\n    project_name: str,\n    interval: datetime.timedelta,\n    version: Union[int, str],\n    inject_params: Mapping = {}\n)", "funcdef": "def"}, {"fullname": "etna.etls.etl_task_batching", "modulename": "etna.etls.etl_task_batching", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.etls.etl_task_batching.get_batch_range", "modulename": "etna.etls.etl_task_batching", "qualname": "get_batch_range", "type": "function", "doc": "<p></p>\n", "signature": "(\n    context: airflow.utils.context.Context\n) -> Tuple[datetime.datetime, datetime.datetime]", "funcdef": "def"}, {"fullname": "etna.etls.linkers", "modulename": "etna.etls.linkers", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.etls.linkers.single_cell_rna_seq", "modulename": "etna.etls.linkers.single_cell_rna_seq", "type": "module", "doc": "<p>Module container helper functions for processing metis files associated with both raw and processed single cell data,\nboth in pools and non pools.  Notably, these functions are versioned to account for changes in the pipeline that may not\nnecessaril ybe backported to all data sets.</p>\n\n<p>Most of these helpers are accessible on the main MetisEtlHelper class as methods.</p>\n"}, {"fullname": "etna.etls.linkers.single_cell_rna_seq.create_and_link_parents", "modulename": "etna.etls.linkers.single_cell_rna_seq", "qualname": "create_and_link_parents", "type": "function", "doc": "<p></p>\n", "signature": "(matches: List[etna.etls.metis.MatchedRecordFolder])", "funcdef": "def"}, {"fullname": "etna.etls.linkers.single_cell_rna_seq.link_single_cell_attribute_files_v1", "modulename": "etna.etls.linkers.single_cell_rna_seq", "qualname": "link_single_cell_attribute_files_v1", "type": "function", "doc": "<p></p>\n", "signature": "(\n    helpers: etna.etls.metis.MetisEtlHelpers,\n    model_name: str,\n    identifier_prefix: str,\n    single_cell_root_prefix='single_cell_',\n    attribute_linker_overrides: Union[Mapping[str, str], NoneType] = None,\n    link_parents=<function create_and_link_parents>,\n    dry_run=True\n) -> Tuple[airflow.models.xcom_arg.XComArg, airflow.models.xcom_arg.XComArg]", "funcdef": "def"}, {"fullname": "etna.etls.metis", "modulename": "etna.etls.metis", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.etls.metis.MatchedRecordFolder", "modulename": "etna.etls.metis", "qualname": "MatchedRecordFolder", "type": "class", "doc": "<p>MatchedRecordFolder(root_path: str, record_name: str, model_name: str, match_file: Union[etna.hooks.etna.File, NoneType] = None, match_folder: Union[etna.hooks.etna.Folder, NoneType] = None, match_parent_raw: Union[Any, NoneType] = None)</p>\n"}, {"fullname": "etna.etls.metis.MatchedRecordFolder.__init__", "modulename": "etna.etls.metis", "qualname": "MatchedRecordFolder.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    root_path: str,\n    record_name: str,\n    model_name: str,\n    match_file: Union[etna.hooks.etna.File, NoneType] = None,\n    match_folder: Union[etna.hooks.etna.Folder, NoneType] = None,\n    match_parent_raw: Union[Any, NoneType] = None\n)", "funcdef": "def"}, {"fullname": "etna.etls.metis.MatchedRecordFolder.match_file", "modulename": "etna.etls.metis", "qualname": "MatchedRecordFolder.match_file", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[etna.hooks.etna.File, NoneType]", "default_value": " = None"}, {"fullname": "etna.etls.metis.MatchedRecordFolder.match_folder", "modulename": "etna.etls.metis", "qualname": "MatchedRecordFolder.match_folder", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[etna.hooks.etna.Folder, NoneType]", "default_value": " = None"}, {"fullname": "etna.etls.metis.MatchedRecordFolder.match_parent_raw", "modulename": "etna.etls.metis", "qualname": "MatchedRecordFolder.match_parent_raw", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[Any, NoneType]", "default_value": " = None"}, {"fullname": "etna.etls.metis.MatchedRecordFolder.match_parent", "modulename": "etna.etls.metis", "qualname": "MatchedRecordFolder.match_parent", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[etna.etls.metis.MatchedRecordFolder, NoneType]"}, {"fullname": "etna.etls.metis.MatchedRecordFolder.updated_at", "modulename": "etna.etls.metis", "qualname": "MatchedRecordFolder.updated_at", "type": "variable", "doc": "<p></p>\n"}, {"fullname": "etna.etls.metis.MatchedRecordFolder.project_name", "modulename": "etna.etls.metis", "qualname": "MatchedRecordFolder.project_name", "type": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "etna.etls.metis.MatchedRecordFolder.as_update", "modulename": "etna.etls.metis", "qualname": "MatchedRecordFolder.as_update", "type": "function", "doc": "<p></p>\n", "signature": "(self, attrs: Dict[str, Any])", "funcdef": "def"}, {"fullname": "etna.etls.metis.MatchedRecordFolder.bucket_name", "modulename": "etna.etls.metis", "qualname": "MatchedRecordFolder.bucket_name", "type": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "etna.etls.metis.MatchedRecordFolder.folder_path", "modulename": "etna.etls.metis", "qualname": "MatchedRecordFolder.folder_path", "type": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "etna.etls.metis.MatchedRecordFolder.folder_id", "modulename": "etna.etls.metis", "qualname": "MatchedRecordFolder.folder_id", "type": "variable", "doc": "<p>The folder id that was matched inside of a record folder.\nNote, this is NOT necessarily the id of the record folder itself, but either</p>\n\n<ol>\n<li>the id of the folder found inside the record folder match or</li>\n<li>the id of the parent folder of the file found inside the record folder (which, could be the record folder)</li>\n</ol>\n", "annotation": ": int"}, {"fullname": "etna.etls.metis.MatchedRecordFolder.match_folder_subpath", "modulename": "etna.etls.metis", "qualname": "MatchedRecordFolder.match_folder_subpath", "type": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "etna.etls.metis.MatchedRecordFolder.match_subpath", "modulename": "etna.etls.metis", "qualname": "MatchedRecordFolder.match_subpath", "type": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "etna.etls.metis.MatchedRecordFolder.match_full_path", "modulename": "etna.etls.metis", "qualname": "MatchedRecordFolder.match_full_path", "type": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "etna.etls.metis.filter_args_for_processed_files", "modulename": "etna.etls.metis", "qualname": "filter_args_for_processed_files", "type": "function", "doc": "<p></p>\n", "signature": "(args: Iterable[Any]) -> Iterable[Any]", "funcdef": "def"}, {"fullname": "etna.etls.metis.link", "modulename": "etna.etls.metis", "qualname": "link", "type": "class", "doc": "<p></p>\n"}, {"fullname": "etna.etls.metis.link.__init__", "modulename": "etna.etls.metis", "qualname": "link.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    attribute_name: Union[str, NoneType] = None,\n    dry_run=True,\n    hook: Union[etna.hooks.etna.EtnaHook, NoneType] = None,\n    task_id: Union[str, NoneType] = None,\n    validate_record_update: Union[Callable[[Any, Any, List[str]], bool], NoneType] = None,\n    batch_size=30\n)", "funcdef": "def"}, {"fullname": "etna.etls.metis.link.project_name", "modulename": "etna.etls.metis", "qualname": "link.project_name", "type": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "etna.etls.metis.link.hook", "modulename": "etna.etls.metis", "qualname": "link.hook", "type": "variable", "doc": "<p></p>\n"}, {"fullname": "etna.etls.metis.MetisEtlHelpers", "modulename": "etna.etls.metis", "qualname": "MetisEtlHelpers", "type": "class", "doc": "<p></p>\n"}, {"fullname": "etna.etls.metis.MetisEtlHelpers.__init__", "modulename": "etna.etls.metis", "qualname": "MetisEtlHelpers.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    tail_folders: airflow.models.xcom_arg.XComArg,\n    tail_files: airflow.models.xcom_arg.XComArg,\n    hook: etna.hooks.etna.EtnaHook\n)", "funcdef": "def"}, {"fullname": "etna.etls.metis.MetisEtlHelpers.prepare_task_token", "modulename": "etna.etls.metis", "qualname": "MetisEtlHelpers.prepare_task_token", "type": "function", "doc": "<p>Returns an XComArg that can be used to pass a task token into other tasks.\nGenerally, this is used with <code>run_on_docker</code> to pass a task token into an input\nfile.</p>\n", "signature": "(self, read_only: bool) -> airflow.models.xcom_arg.XComArg", "funcdef": "def"}, {"fullname": "etna.etls.metis.MetisEtlHelpers.process_and_link_matching_file", "modulename": "etna.etls.metis", "qualname": "MetisEtlHelpers.process_and_link_matching_file", "type": "function", "doc": "<p>Creates a task that follows the listed set of record match folders and calls the given\nprocessor function to provide an UpdateRequest object as a result of matching files from\nthe listed match record folder.</p>\n\n<p>The decorated function can receive arguments named metis, file, match, or template,\neach being either a MetisClient, File, MatchedRecordFolder, or Template respectively.</p>\n\n<p>eg:</p>\n\n<pre><code>def my_processor(metis, template, match, file):\n  with metis.open_file(file) as file_reader:\n    csv_reader = csv.reader(file_reader)\n    return match.as_update(csv_reader)\n\nmatches = helpers.find_record_folders('rna_seq', rna_seq_folder_regex)\nlisted_matches = helpers.list_match_folders(matches)\nhelpers.process_and_link_matching_file(listed_matches, re.compile(r'.*gene_counts\\.tsv$'), my_processor)\n</code></pre>\n", "signature": "(\n    self,\n    listed_record_matches: airflow.models.xcom_arg.XComArg,\n    regex: re.Pattern,\n    processor: Callable[[], etna.hooks.etna.UpdateRequest],\n    dry_run=True\n) -> airflow.models.xcom_arg.XComArg", "funcdef": "def"}, {"fullname": "etna.etls.metis.MetisEtlHelpers.link_single_cell_attribute_files_v1", "modulename": "etna.etls.metis", "qualname": "MetisEtlHelpers.link_single_cell_attribute_files_v1", "type": "function", "doc": "<p>Links single cell rna related attributes for either sc_rna_seq or sc_rna_seq_pool models, including\nraw fastqs and processed files.</p>\n\n<p>Checkout <code>etna.etls.linkers.signle_cell_rna_seq.link_single_cell_attribute_files_v1</code> source code\nfor more information on how matches are made and which attributes are linked.</p>\n", "signature": "(\n    self,\n    model_name: str,\n    identifier_prefix: str,\n    single_cell_root_prefix='single_cell_',\n    attribute_linker_overrides: Union[Mapping[str, str], NoneType] = None,\n    link_parents: Union[Callable[[List[etna.etls.metis.MatchedRecordFolder]], Iterable[Tuple[etna.etls.metis.MatchedRecordFolder, etna.hooks.etna.UpdateRequest]]], NoneType] = None,\n    dry_run=True\n) -> Tuple[airflow.models.xcom_arg.XComArg, airflow.models.xcom_arg.XComArg]", "funcdef": "def"}, {"fullname": "etna.etls.metis.MetisEtlHelpers.link_matching_file", "modulename": "etna.etls.metis", "qualname": "MetisEtlHelpers.link_matching_file", "type": "function", "doc": "<p>Creates a task that iterates the provided listed_record_matches (the result of a helpers.list_match_folders\ncall), searching for files whose file_name matches the given regex, linking the file path to the given\nmatch's record in magma at the given attr_name attribute.</p>\n\n<p>Notably, when multiple files are found, the -last- file detected will be used in magma.  If you intend to link\nseveral matching files, use link_matching_files for attributes that are file_collection.</p>\n", "signature": "(\n    self,\n    listed_record_matches: airflow.models.xcom_arg.XComArg,\n    attr_name: str,\n    regex: re.Pattern,\n    dry_run=True\n) -> airflow.models.xcom_arg.XComArg", "funcdef": "def"}, {"fullname": "etna.etls.metis.MetisEtlHelpers.link_matching_files", "modulename": "etna.etls.metis", "qualname": "MetisEtlHelpers.link_matching_files", "type": "function", "doc": "<p>Like link_matching_file, except for two key points:</p>\n\n<ol>\n<li>all matches found are linked together as a file collection</li>\n<li>An additional argument, folder_path_regex, is matched against the folder before linking.\nBecause listed_record_matches ONLY sees a single directory's contents at a time, this\nfolder_path_regex ensures the correct folder is selected before linking, in case multiple\nchild directories exist for a matched record folder.</li>\n</ol>\n", "signature": "(\n    self,\n    listed_record_matches: airflow.models.xcom_arg.XComArg,\n    attr_name: str,\n    file_regex: re.Pattern,\n    folder_path_regex: re.Pattern = re.compile('^$'),\n    dry_run=True\n) -> airflow.models.xcom_arg.XComArg", "funcdef": "def"}, {"fullname": "etna.etls.metis.MetisEtlHelpers.link_record_folders_to_parent", "modulename": "etna.etls.metis", "qualname": "MetisEtlHelpers.link_record_folders_to_parent", "type": "function", "doc": "<p>When record folders are embedded in metis such that parent model record folders\nexist, it is common to want to 'link' the parent model attribute based on the folder name.</p>\n\n<p>This function takes an XComArg containing a list of MatchedRecordFolder obtained via\n<code>find_record_folders</code>, <em>whose source arg was not None</em>.  In other words, consider this example:</p>\n\n<p>Imagine a directory structure like /MYSAMPLE.T1/ScRnaSeq/MYSAMPLE.T1.BLAH.BLAH</p>\n\n<pre><code>sample_matches = helpers.find_record_folders(\"sample\", SAMPLE_REGEX)\nwith TaskGroup(\"sc_rna_seq_matches\"):\n  sc_rna_seq_matches = helpers.find_record_folders(\"sc_rna_seq\", SC_RNA_SEQ_REGEX, source=sample_matches)\n  helpers.link_record_folders_to_parent(sc_rna_esq_matches)\n</code></pre>\n\n<p>In this case, FIRST task will find \"/MYSAMPLE.T1\" as a \"sample\" record folder and it will belong\nto the <code>XComArg</code> returned as <code>sample_matches</code>.</p>\n\n<p>Next, it will search under /MYSAMPLE.T1 to find the MYSAMPLE.T1.BLAH.BLAH directory as a \"sc_rna_eq\" model\ndirectory, noting the parent record folder as MYSAMPLE.T1 and that pairing will belong to sc_rna_seq_matches.</p>\n\n<p>Lastly, the <code>link_record_folders_to_parent</code> will use the association between MYSAMPLE.T1.BLAH.BLAH and MYSAMPLE.T1\nto make the association between that sc_rna_seq record and the parent sample record.</p>\n", "signature": "(\n    self,\n    embedded_matches: airflow.models.xcom_arg.XComArg,\n    dry_run=True\n)", "funcdef": "def"}, {"fullname": "etna.etls.metis.MetisEtlHelpers.find_record_folders", "modulename": "etna.etls.metis", "qualname": "MetisEtlHelpers.find_record_folders", "type": "function", "doc": "<p>Creates a task that Searches for folders that match the given a regex creates MatchedRecordFolder\nentries with the name of the directory at the far end of the match and the given model name.  The match must\noccur at a folder boundary, so for instance, r'abc/def' does not match 'abc/defg' but does match 'abc/def/c'.\nThe directory matched by the last path segment of the regex is considered a 'record name' belonging to the given\nmodel_name, allowing other convenience methods to process and filter with that assumption.</p>\n\n<p><code>source</code> should be used when processing inner record folders, with the outer record matches XComArg being passed.\nIn that case, you should use a <code>with TaskGroup('---')</code> surrounding this task to distinguish the inner task name\nfrom the outer one.</p>\n", "signature": "(\n    self,\n    model_name: str,\n    regex: re.Pattern,\n    corrected_record_name: Callable[[str], str] = <function MetisEtlHelpers.<lambda>>,\n    source: Union[airflow.models.xcom_arg.XComArg, NoneType] = None\n) -> airflow.models.xcom_arg.XComArg", "funcdef": "def"}, {"fullname": "etna.etls.metis.MetisEtlHelpers.filter_by_timur", "modulename": "etna.etls.metis", "qualname": "MetisEtlHelpers.filter_by_timur", "type": "function", "doc": "<p>Creates a task that first validates the model / record name of the provided record folder matches already exist\nin magma / timur, ensuring that any incorrectly named folder does not accidentally become a magma record.</p>\n", "signature": "(\n    self,\n    matches: airflow.models.xcom_arg.XComArg\n) -> airflow.models.xcom_arg.XComArg", "funcdef": "def"}, {"fullname": "etna.etls.metis.MetisEtlHelpers.list_match_folders", "modulename": "etna.etls.metis", "qualname": "MetisEtlHelpers.list_match_folders", "type": "function", "doc": "<p>Creates a task that lists the contents of the given MatchedRecordFolders.  For instance, if a single file\nis added to a record folder, the entire contents of that record folder are listed for use by downstream\nlinkers.  this is important when linking multiple files in a collection, or processing that involves\nreading in multiple files that exist in the same record folder.</p>\n\n<p>The resulting XComArg will provide a value to any consuming task a value of List[Tuple[MatchedRecordFolder, List[File]]]</p>\n", "signature": "(self, matches: airflow.models.xcom_arg.XComArg)", "funcdef": "def"}, {"fullname": "etna.etls.metis.processed_file_variable_key", "modulename": "etna.etls.metis", "qualname": "processed_file_variable_key", "type": "function", "doc": "<p></p>\n", "signature": "() -> str", "funcdef": "def"}, {"fullname": "etna.etls.metis.process_file_set", "modulename": "etna.etls.metis", "qualname": "process_file_set", "type": "function", "doc": "<p></p>\n", "signature": "() -> List[str]", "funcdef": "def"}, {"fullname": "etna.etls.metis.mark_processed_file", "modulename": "etna.etls.metis", "qualname": "mark_processed_file", "type": "function", "doc": "<p></p>\n", "signature": "(file: etna.hooks.etna.File)", "funcdef": "def"}, {"fullname": "etna.etls.metis.filter_by_exists_in_timur", "modulename": "etna.etls.metis", "qualname": "filter_by_exists_in_timur", "type": "function", "doc": "<p>Underlying function that implements helpers.filter_by_timur, takes a magma client and a list of matched record folders,\nand returns the list of matches that have a backing record in magma.</p>\n", "signature": "(\n    magma: etna.hooks.etna.Magma,\n    matched: List[etna.etls.metis.MatchedRecordFolder]\n) -> List[etna.etls.metis.MatchedRecordFolder]", "funcdef": "def"}, {"fullname": "etna.etls.metis.filter_by_record_directory", "modulename": "etna.etls.metis", "qualname": "filter_by_record_directory", "type": "function", "doc": "<p>Underlying implementation that backs helpers.find_record_folders, searching a list of File and Folder objects\nfor any whose file_path matches the directory regex, and creating MatchedRecordFolders for each unique folder_path\nfound in this way.</p>\n", "signature": "(\n    files_or_folders: List[Union[etna.hooks.etna.File, etna.hooks.etna.Folder, etna.etls.metis.MatchedRecordFolder]],\n    directory_regex: re.Pattern,\n    model_name: str,\n    corrected_record_name: Callable[[str], str] = <function <lambda>>\n) -> List[etna.etls.metis.MatchedRecordFolder]", "funcdef": "def"}, {"fullname": "etna.etls.metis.load_metis_files_batch", "modulename": "etna.etls.metis", "qualname": "load_metis_files_batch", "type": "function", "doc": "<p></p>\n", "signature": "(\n    metis: etna.hooks.etna.Metis,\n    bucket_name: str,\n    project_name: Union[str, NoneType] = None\n) -> List[etna.hooks.etna.File]", "funcdef": "def"}, {"fullname": "etna.etls.metis.load_metis_folders_batch", "modulename": "etna.etls.metis", "qualname": "load_metis_folders_batch", "type": "function", "doc": "<p></p>\n", "signature": "(\n    metis: etna.hooks.etna.Metis,\n    bucket_name: str,\n    project_name: Union[str, NoneType] = None\n) -> List[etna.hooks.etna.Folder]", "funcdef": "def"}, {"fullname": "etna.hooks", "modulename": "etna.hooks", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.hooks.box", "modulename": "etna.hooks.box", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.hooks.box.BoxHook", "modulename": "etna.hooks.box", "qualname": "BoxHook", "type": "class", "doc": "<p>Box Client Hook to manage the connection</p>\n", "bases": "airflow.hooks.base.BaseHook"}, {"fullname": "etna.hooks.box.BoxHook.__init__", "modulename": "etna.hooks.box", "qualname": "BoxHook.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, box_conn_id: str)", "funcdef": "def"}, {"fullname": "etna.hooks.box.BoxHook.conn_name_attr", "modulename": "etna.hooks.box", "qualname": "BoxHook.conn_name_attr", "type": "variable", "doc": "<p></p>\n", "default_value": " = 'box_conn_id'"}, {"fullname": "etna.hooks.box.BoxHook.default_conn_name", "modulename": "etna.hooks.box", "qualname": "BoxHook.default_conn_name", "type": "variable", "doc": "<p></p>\n", "default_value": " = 'box_default'"}, {"fullname": "etna.hooks.box.BoxHook.conn_type", "modulename": "etna.hooks.box", "qualname": "BoxHook.conn_type", "type": "variable", "doc": "<p></p>\n", "default_value": " = 'box'"}, {"fullname": "etna.hooks.box.BoxHook.hook_name", "modulename": "etna.hooks.box", "qualname": "BoxHook.hook_name", "type": "variable", "doc": "<p></p>\n", "default_value": " = 'Box Connection'"}, {"fullname": "etna.hooks.box.BoxHook.root_dir", "modulename": "etna.hooks.box", "qualname": "BoxHook.root_dir", "type": "variable", "doc": "<p></p>\n", "default_value": " = '/opt/airflow/dags/repos'"}, {"fullname": "etna.hooks.box.BoxHook.get_ui_field_behaviour", "modulename": "etna.hooks.box", "qualname": "BoxHook.get_ui_field_behaviour", "type": "function", "doc": "<p></p>\n", "signature": "() -> Dict", "funcdef": "def"}, {"fullname": "etna.hooks.box.BoxHook.get_connection_form_widgets", "modulename": "etna.hooks.box", "qualname": "BoxHook.get_connection_form_widgets", "type": "function", "doc": "<p>Returns dictionary of widgets to be added for the hook to handle extra values.</p>\n\n<p>If you have class hierarchy, usually the widgets needed by your class are already\nadded by the base class, so there is no need to implement this method. It might\nactually result in warning in the logs if you try to add widgets that have already\nbeen added by the base class.</p>\n\n<p>Note that values of Dict should be of wtforms.Field type. It's not added here\nfor the efficiency of imports.</p>\n", "signature": "() -> Dict", "funcdef": "def"}, {"fullname": "etna.hooks.box.BoxHook.for_project", "modulename": "etna.hooks.box", "qualname": "BoxHook.for_project", "type": "function", "doc": "<p></p>\n", "signature": "(cls, project_name: Union[str, NoneType] = None)", "funcdef": "def"}, {"fullname": "etna.hooks.box.BoxHook.get_conn", "modulename": "etna.hooks.box", "qualname": "BoxHook.get_conn", "type": "function", "doc": "<p>Returns connection for the hook.</p>\n", "signature": "(self) -> airflow.models.connection.Connection", "funcdef": "def"}, {"fullname": "etna.hooks.box.BoxHook.connection", "modulename": "etna.hooks.box", "qualname": "BoxHook.connection", "type": "function", "doc": "<p></p>\n", "signature": "(unknown)", "funcdef": "def"}, {"fullname": "etna.hooks.box.BoxHook.box", "modulename": "etna.hooks.box", "qualname": "BoxHook.box", "type": "function", "doc": "<p></p>\n", "signature": "(self) -> contextlib.AbstractContextManager[etna.hooks.box.Box]", "funcdef": "def"}, {"fullname": "etna.hooks.box.FtpEntry", "modulename": "etna.hooks.box", "qualname": "FtpEntry", "type": "class", "doc": "<p></p>\n"}, {"fullname": "etna.hooks.box.FtpEntry.__init__", "modulename": "etna.hooks.box", "qualname": "FtpEntry.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, tuple, parent_path: str)", "funcdef": "def"}, {"fullname": "etna.hooks.box.FtpEntry.size", "modulename": "etna.hooks.box", "qualname": "FtpEntry.size", "type": "variable", "doc": "<p></p>\n", "annotation": ": int"}, {"fullname": "etna.hooks.box.FtpEntry.is_file", "modulename": "etna.hooks.box", "qualname": "FtpEntry.is_file", "type": "function", "doc": "<p></p>\n", "signature": "(self) -> bool", "funcdef": "def"}, {"fullname": "etna.hooks.box.FtpEntry.is_dir", "modulename": "etna.hooks.box", "qualname": "FtpEntry.is_dir", "type": "function", "doc": "<p></p>\n", "signature": "(self) -> bool", "funcdef": "def"}, {"fullname": "etna.hooks.box.FtpEntry.mtime", "modulename": "etna.hooks.box", "qualname": "FtpEntry.mtime", "type": "variable", "doc": "<p></p>\n", "annotation": ": datetime.datetime"}, {"fullname": "etna.hooks.box.FtpEntry.is_dot", "modulename": "etna.hooks.box", "qualname": "FtpEntry.is_dot", "type": "function", "doc": "<p></p>\n", "signature": "(self) -> bool", "funcdef": "def"}, {"fullname": "etna.hooks.box.FtpEntry.full_path", "modulename": "etna.hooks.box", "qualname": "FtpEntry.full_path", "type": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "etna.hooks.box.FtpEntry.rel_path", "modulename": "etna.hooks.box", "qualname": "FtpEntry.rel_path", "type": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "etna.hooks.box.FtpEntry.is_in_range", "modulename": "etna.hooks.box", "qualname": "FtpEntry.is_in_range", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    batch_start: Union[datetime.datetime, NoneType] = None,\n    batch_end: Union[datetime.datetime, NoneType] = None\n)", "funcdef": "def"}, {"fullname": "etna.hooks.box.FtpEntry.hash", "modulename": "etna.hooks.box", "qualname": "FtpEntry.hash", "type": "variable", "doc": "<p></p>\n"}, {"fullname": "etna.hooks.box.Box", "modulename": "etna.hooks.box", "qualname": "Box", "type": "class", "doc": "<p></p>\n"}, {"fullname": "etna.hooks.box.Box.__init__", "modulename": "etna.hooks.box", "qualname": "Box.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, hook: etna.hooks.box.BoxHook)", "funcdef": "def"}, {"fullname": "etna.hooks.box.Box.variable_root", "modulename": "etna.hooks.box", "qualname": "Box.variable_root", "type": "variable", "doc": "<p></p>\n", "default_value": " = 'box_ingest_cursor'"}, {"fullname": "etna.hooks.box.Box.valid_folder_name", "modulename": "etna.hooks.box", "qualname": "Box.valid_folder_name", "type": "function", "doc": "<p></p>\n", "signature": "(cls, folder_name: str)", "funcdef": "def"}, {"fullname": "etna.hooks.box.Box.tail", "modulename": "etna.hooks.box", "qualname": "Box.tail", "type": "function", "doc": "<p>Tails all files found in the given <code>folder_name</code>, that have not been ingested previously.\n    Recursively searches sub-folders.</p>\n\n<p>params:\n  folder_name: str, the top-level folder to search from</p>\n", "signature": "(self, folder_name: str) -> List[etna.hooks.box.FtpEntry]", "funcdef": "def"}, {"fullname": "etna.hooks.box.Box.ftps", "modulename": "etna.hooks.box", "qualname": "Box.ftps", "type": "function", "doc": "<p>Configures an FTP_TLS connection to Box. Using Python <code>with</code> syntax, so that the\nconnection is closed after usage.</p>\n\n<p>eg:</p>\n\n<pre><code>with box.ftps() as ftps:\n    ftps.sendcmd(\"LIST\")\n</code></pre>\n", "signature": "(self) -> ftplib.FTP_TLS", "funcdef": "def"}, {"fullname": "etna.hooks.box.Box.retrieve_file", "modulename": "etna.hooks.box", "qualname": "Box.retrieve_file", "type": "function", "doc": "<p>Opens the given file for download into a context as a Tuple(BinaryIO, size).\nThe underlying socket object yields bytes objects.</p>\n\n<p>Note:  Ideally, this method is used in combination with 'with' syntax in python, so that the underlying\ndata stream is closed after usage.  This is especially performant when code only needs to access a small\nsubset of the data.</p>\n\n<p>eg:</p>\n\n<pre><code>socket = box.retrieve_file(file)\nwith socket.makefile('rb') as connection:\n    for line in csv.reader(connection):\n        break\n</code></pre>\n\n<p>params:\n  ftps: an open, FTP_TLS connection\n  file: an FTP file listing</p>\n", "signature": "(\n    self,\n    ftps: ftplib.FTP_TLS,\n    file: etna.hooks.box.FtpEntry\n) -> Tuple[BinaryIO, int]", "funcdef": "def"}, {"fullname": "etna.hooks.box.Box.mark_file_as_ingested", "modulename": "etna.hooks.box", "qualname": "Box.mark_file_as_ingested", "type": "function", "doc": "<p>In the cursor, save the fact that the given file's upload was completed.</p>\n", "signature": "(self, file: etna.hooks.box.FtpEntry)", "funcdef": "def"}, {"fullname": "etna.hooks.box.Box.update_cursor", "modulename": "etna.hooks.box", "qualname": "Box.update_cursor", "type": "function", "doc": "<p>Save the cursor to the database.</p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "etna.hooks.box.Box.remove_file", "modulename": "etna.hooks.box", "qualname": "Box.remove_file", "type": "function", "doc": "<p>Removes the file from the FTP server, if user wants to automatically\nclean up after ingestion to Metis.</p>\n", "signature": "(self, ftps: ftplib.FTP_TLS, file: etna.hooks.box.FtpEntry)", "funcdef": "def"}, {"fullname": "etna.hooks.box.Box.variable_key", "modulename": "etna.hooks.box", "qualname": "Box.variable_key", "type": "variable", "doc": "<p>Return the variable key for the current dag.</p>\n"}, {"fullname": "etna.hooks.connections", "modulename": "etna.hooks.connections", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.hooks.connections.find_first_valid_connection", "modulename": "etna.hooks.connections", "qualname": "find_first_valid_connection", "type": "function", "doc": "<p></p>\n", "signature": "(*options: Union[str, NoneType]) -> str", "funcdef": "def"}, {"fullname": "etna.hooks.etna", "modulename": "etna.hooks.etna", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.hooks.etna.EtnaHook", "modulename": "etna.hooks.etna", "qualname": "EtnaHook", "type": "class", "doc": "<p>Etna Client Hook that can generate task tokens</p>\n", "bases": "airflow.hooks.base.BaseHook"}, {"fullname": "etna.hooks.etna.EtnaHook.__init__", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, etna_conn_id: str)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.EtnaHook.conn_name_attr", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.conn_name_attr", "type": "variable", "doc": "<p></p>\n", "default_value": " = 'etna_conn_id'"}, {"fullname": "etna.hooks.etna.EtnaHook.default_conn_name", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.default_conn_name", "type": "variable", "doc": "<p></p>\n", "default_value": " = 'etna_default'"}, {"fullname": "etna.hooks.etna.EtnaHook.conn_type", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.conn_type", "type": "variable", "doc": "<p></p>\n", "default_value": " = 'etna'"}, {"fullname": "etna.hooks.etna.EtnaHook.hook_name", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.hook_name", "type": "variable", "doc": "<p></p>\n", "default_value": " = 'Etna Connection'"}, {"fullname": "etna.hooks.etna.EtnaHook.root_dir", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.root_dir", "type": "variable", "doc": "<p></p>\n", "default_value": " = '/opt/airflow/dags/repos'"}, {"fullname": "etna.hooks.etna.EtnaHook.get_ui_field_behaviour", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.get_ui_field_behaviour", "type": "function", "doc": "<p></p>\n", "signature": "() -> Dict", "funcdef": "def"}, {"fullname": "etna.hooks.etna.EtnaHook.get_connection_form_widgets", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.get_connection_form_widgets", "type": "function", "doc": "<p>Returns dictionary of widgets to be added for the hook to handle extra values.</p>\n\n<p>If you have class hierarchy, usually the widgets needed by your class are already\nadded by the base class, so there is no need to implement this method. It might\nactually result in warning in the logs if you try to add widgets that have already\nbeen added by the base class.</p>\n\n<p>Note that values of Dict should be of wtforms.Field type. It's not added here\nfor the efficiency of imports.</p>\n", "signature": "() -> Dict", "funcdef": "def"}, {"fullname": "etna.hooks.etna.EtnaHook.for_project", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.for_project", "type": "function", "doc": "<p></p>\n", "signature": "(cls, project_name: Union[str, NoneType] = None)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.EtnaHook.get_conn", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.get_conn", "type": "function", "doc": "<p>Returns connection for the hook.</p>\n", "signature": "(self) -> airflow.models.connection.Connection", "funcdef": "def"}, {"fullname": "etna.hooks.etna.EtnaHook.connection", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.connection", "type": "function", "doc": "<p></p>\n", "signature": "(unknown)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.EtnaHook.get_hostname", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.get_hostname", "type": "function", "doc": "<p></p>\n", "signature": "(self, host_prefix: str)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.EtnaHook.get_token_auth", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.get_token_auth", "type": "function", "doc": "<p></p>\n", "signature": "(self) -> requests.auth.AuthBase", "funcdef": "def"}, {"fullname": "etna.hooks.etna.EtnaHook.get_task_auth", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.get_task_auth", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    project_name: Union[str, NoneType] = None,\n    read_only=True\n) -> etna.hooks.etna.TokenAuth", "funcdef": "def"}, {"fullname": "etna.hooks.etna.EtnaHook.get_client", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.get_client", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    auth: Union[requests.auth.AuthBase, NoneType]\n) -> AbstractContextManager[requests.sessions.Session]", "funcdef": "def"}, {"fullname": "etna.hooks.etna.EtnaHook.metis", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.metis", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    project_name: Union[str, NoneType] = None,\n    read_only=True\n) -> contextlib.AbstractContextManager[etna.hooks.etna.Metis]", "funcdef": "def"}, {"fullname": "etna.hooks.etna.EtnaHook.janus", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.janus", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    project_name: Union[str, NoneType] = None,\n    read_only=True\n) -> contextlib.AbstractContextManager[etna.hooks.etna.Janus]", "funcdef": "def"}, {"fullname": "etna.hooks.etna.EtnaHook.magma", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.magma", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    project_name: Union[str, NoneType] = None,\n    read_only=True\n) -> contextlib.AbstractContextManager[etna.hooks.etna.Magma]", "funcdef": "def"}, {"fullname": "etna.hooks.etna.EtnaHook.assert_status", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.assert_status", "type": "function", "doc": "<p></p>\n", "signature": "(response: requests.models.Response, *args, **kwds)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.EtnaHook.generate_task_token", "modulename": "etna.hooks.etna", "qualname": "EtnaHook.generate_task_token", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.TokenAuth", "modulename": "etna.hooks.etna", "qualname": "TokenAuth", "type": "class", "doc": "<p>Base class that all auth implementations derive from</p>\n", "bases": "requests.auth.AuthBase"}, {"fullname": "etna.hooks.etna.TokenAuth.__init__", "modulename": "etna.hooks.etna", "qualname": "TokenAuth.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, token: bytes, project_scope: Union[str, NoneType] = None)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.SigAuth", "modulename": "etna.hooks.etna", "qualname": "SigAuth", "type": "class", "doc": "<p>Base class that all auth implementations derive from</p>\n", "bases": "requests.auth.AuthBase"}, {"fullname": "etna.hooks.etna.SigAuth.__init__", "modulename": "etna.hooks.etna", "qualname": "SigAuth.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, private_key_file: str, email: str, nonce: bytes)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.encode_path", "modulename": "etna.hooks.etna", "qualname": "encode_path", "type": "function", "doc": "<p></p>\n", "signature": "(*segments: str) -> str", "funcdef": "def"}, {"fullname": "etna.hooks.etna.EtnaClientBase", "modulename": "etna.hooks.etna", "qualname": "EtnaClientBase", "type": "class", "doc": "<p></p>\n"}, {"fullname": "etna.hooks.etna.EtnaClientBase.__init__", "modulename": "etna.hooks.etna", "qualname": "EtnaClientBase.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, session: requests.sessions.Session, hostname: str)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.EtnaClientBase.auth_token", "modulename": "etna.hooks.etna", "qualname": "EtnaClientBase.auth_token", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[bytes, NoneType]"}, {"fullname": "etna.hooks.etna.EtnaClientBase.prepare_url", "modulename": "etna.hooks.etna", "qualname": "EtnaClientBase.prepare_url", "type": "function", "doc": "<p></p>\n", "signature": "(self, *path: str)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.EtnaClientBase.prepare_url_unsafe", "modulename": "etna.hooks.etna", "qualname": "EtnaClientBase.prepare_url_unsafe", "type": "function", "doc": "<p></p>\n", "signature": "(self, path: str)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.EtnaClientBase.get_project_scope", "modulename": "etna.hooks.etna", "qualname": "EtnaClientBase.get_project_scope", "type": "function", "doc": "<p></p>\n", "signature": "(self) -> Union[str, NoneType]", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Janus", "modulename": "etna.hooks.etna", "qualname": "Janus", "type": "class", "doc": "<p></p>\n", "bases": "EtnaClientBase"}, {"fullname": "etna.hooks.etna.Janus.generate_token", "modulename": "etna.hooks.etna", "qualname": "Janus.generate_token", "type": "function", "doc": "<p></p>\n", "signature": "(self, project_name: str, read_only=True, token_type='task') -> bytes", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Janus.nonce", "modulename": "etna.hooks.etna", "qualname": "Janus.nonce", "type": "function", "doc": "<p></p>\n", "signature": "(self) -> bytes", "funcdef": "def"}, {"fullname": "etna.hooks.etna.MagmaFileEntry", "modulename": "etna.hooks.etna", "qualname": "MagmaFileEntry", "type": "class", "doc": "<p>MagmaFileEntry(path: str = '', original_filename: str = '')</p>\n"}, {"fullname": "etna.hooks.etna.MagmaFileEntry.__init__", "modulename": "etna.hooks.etna", "qualname": "MagmaFileEntry.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, path: str = '', original_filename: str = '')", "funcdef": "def"}, {"fullname": "etna.hooks.etna.MagmaFileEntry.path", "modulename": "etna.hooks.etna", "qualname": "MagmaFileEntry.path", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.MagmaFileEntry.original_filename", "modulename": "etna.hooks.etna", "qualname": "MagmaFileEntry.original_filename", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.TailNode", "modulename": "etna.hooks.etna", "qualname": "TailNode", "type": "class", "doc": "<p>TailNode(id: int = 0, type: str = 'file', node_name: str = '', updated_at: str = '', parent_id: Union[int, NoneType] = None, file_hash: Union[str, NoneType] = None, archive_id: Union[int, NoneType] = None)</p>\n"}, {"fullname": "etna.hooks.etna.TailNode.__init__", "modulename": "etna.hooks.etna", "qualname": "TailNode.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    id: int = 0,\n    type: str = 'file',\n    node_name: str = '',\n    updated_at: str = '',\n    parent_id: Union[int, NoneType] = None,\n    file_hash: Union[str, NoneType] = None,\n    archive_id: Union[int, NoneType] = None\n)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.TailNode.id", "modulename": "etna.hooks.etna", "qualname": "TailNode.id", "type": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": " = 0"}, {"fullname": "etna.hooks.etna.TailNode.type", "modulename": "etna.hooks.etna", "qualname": "TailNode.type", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = 'file'"}, {"fullname": "etna.hooks.etna.TailNode.node_name", "modulename": "etna.hooks.etna", "qualname": "TailNode.node_name", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.TailNode.updated_at", "modulename": "etna.hooks.etna", "qualname": "TailNode.updated_at", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.TailNode.parent_id", "modulename": "etna.hooks.etna", "qualname": "TailNode.parent_id", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[int, NoneType]", "default_value": " = None"}, {"fullname": "etna.hooks.etna.TailNode.file_hash", "modulename": "etna.hooks.etna", "qualname": "TailNode.file_hash", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[str, NoneType]", "default_value": " = None"}, {"fullname": "etna.hooks.etna.TailNode.archive_id", "modulename": "etna.hooks.etna", "qualname": "TailNode.archive_id", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[int, NoneType]", "default_value": " = None"}, {"fullname": "etna.hooks.etna.TailNode.updated_at_datetime", "modulename": "etna.hooks.etna", "qualname": "TailNode.updated_at_datetime", "type": "variable", "doc": "<p></p>\n", "annotation": ": datetime.datetime"}, {"fullname": "etna.hooks.etna.TailResultContainer", "modulename": "etna.hooks.etna", "qualname": "TailResultContainer", "type": "class", "doc": "<p></p>\n"}, {"fullname": "etna.hooks.etna.TailResultContainer.__init__", "modulename": "etna.hooks.etna", "qualname": "TailResultContainer.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, bucket_name: str, project_name: str)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.TailResultContainer.add", "modulename": "etna.hooks.etna", "qualname": "TailResultContainer.add", "type": "function", "doc": "<p></p>\n", "signature": "(self, node: etna.hooks.etna.TailNode)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.TailResultContainer.resolve_files", "modulename": "etna.hooks.etna", "qualname": "TailResultContainer.resolve_files", "type": "function", "doc": "<p></p>\n", "signature": "(self) -> list[etna.hooks.etna.File]", "funcdef": "def"}, {"fullname": "etna.hooks.etna.TailResultContainer.resolve_folders", "modulename": "etna.hooks.etna", "qualname": "TailResultContainer.resolve_folders", "type": "function", "doc": "<p></p>\n", "signature": "(self) -> list[etna.hooks.etna.Folder]", "funcdef": "def"}, {"fullname": "etna.hooks.etna.TailResultContainer.resolve_path", "modulename": "etna.hooks.etna", "qualname": "TailResultContainer.resolve_path", "type": "function", "doc": "<p></p>\n", "signature": "(self, node: etna.hooks.etna.TailNode)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.File", "modulename": "etna.hooks.etna", "qualname": "File", "type": "class", "doc": "<p>File(file_name: str = '', file_hash: str = '', updated_at: str = '', file_path: Union[str, NoneType] = '', folder_id: Union[int, NoneType] = None, project_name: str = '', bucket_name: str = '', download_url: Union[str, NoneType] = '')</p>\n"}, {"fullname": "etna.hooks.etna.File.__init__", "modulename": "etna.hooks.etna", "qualname": "File.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    file_name: str = '',\n    file_hash: str = '',\n    updated_at: str = '',\n    file_path: Union[str, NoneType] = '',\n    folder_id: Union[int, NoneType] = None,\n    project_name: str = '',\n    bucket_name: str = '',\n    download_url: Union[str, NoneType] = ''\n)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.File.file_name", "modulename": "etna.hooks.etna", "qualname": "File.file_name", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.File.file_hash", "modulename": "etna.hooks.etna", "qualname": "File.file_hash", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.File.updated_at", "modulename": "etna.hooks.etna", "qualname": "File.updated_at", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.File.file_path", "modulename": "etna.hooks.etna", "qualname": "File.file_path", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[str, NoneType]", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.File.folder_id", "modulename": "etna.hooks.etna", "qualname": "File.folder_id", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[int, NoneType]", "default_value": " = None"}, {"fullname": "etna.hooks.etna.File.project_name", "modulename": "etna.hooks.etna", "qualname": "File.project_name", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.File.bucket_name", "modulename": "etna.hooks.etna", "qualname": "File.bucket_name", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.File.download_url", "modulename": "etna.hooks.etna", "qualname": "File.download_url", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[str, NoneType]", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.File.as_magma_file_attribute", "modulename": "etna.hooks.etna", "qualname": "File.as_magma_file_attribute", "type": "variable", "doc": "<p></p>\n", "annotation": ": etna.hooks.etna.MagmaFileEntry"}, {"fullname": "etna.hooks.etna.File.as_metis_url", "modulename": "etna.hooks.etna", "qualname": "File.as_metis_url", "type": "variable", "doc": "<p></p>\n"}, {"fullname": "etna.hooks.etna.File.updated_at_datetime", "modulename": "etna.hooks.etna", "qualname": "File.updated_at_datetime", "type": "variable", "doc": "<p></p>\n", "annotation": ": datetime.datetime"}, {"fullname": "etna.hooks.etna.Folder", "modulename": "etna.hooks.etna", "qualname": "Folder", "type": "class", "doc": "<p>Folder(id: int = 0, folder_path: str = '', project_name: str = '', bucket_name: str = '', folder_name: str = '', updated_at: str = '', folder_id: int = 0)</p>\n"}, {"fullname": "etna.hooks.etna.Folder.__init__", "modulename": "etna.hooks.etna", "qualname": "Folder.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    id: int = 0,\n    folder_path: str = '',\n    project_name: str = '',\n    bucket_name: str = '',\n    folder_name: str = '',\n    updated_at: str = '',\n    folder_id: int = 0\n)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Folder.id", "modulename": "etna.hooks.etna", "qualname": "Folder.id", "type": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": " = 0"}, {"fullname": "etna.hooks.etna.Folder.folder_path", "modulename": "etna.hooks.etna", "qualname": "Folder.folder_path", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.Folder.project_name", "modulename": "etna.hooks.etna", "qualname": "Folder.project_name", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.Folder.bucket_name", "modulename": "etna.hooks.etna", "qualname": "Folder.bucket_name", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.Folder.folder_name", "modulename": "etna.hooks.etna", "qualname": "Folder.folder_name", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.Folder.updated_at", "modulename": "etna.hooks.etna", "qualname": "Folder.updated_at", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.Folder.folder_id", "modulename": "etna.hooks.etna", "qualname": "Folder.folder_id", "type": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": " = 0"}, {"fullname": "etna.hooks.etna.Folder.updated_at_datetime", "modulename": "etna.hooks.etna", "qualname": "Folder.updated_at_datetime", "type": "variable", "doc": "<p></p>\n", "annotation": ": datetime.datetime"}, {"fullname": "etna.hooks.etna.FoldersAndFilesResponse", "modulename": "etna.hooks.etna", "qualname": "FoldersAndFilesResponse", "type": "class", "doc": "<p>FoldersAndFilesResponse(folders: List[etna.hooks.etna.Folder] = <factory>, files: List[etna.hooks.etna.File] = <factory>)</p>\n"}, {"fullname": "etna.hooks.etna.FoldersAndFilesResponse.__init__", "modulename": "etna.hooks.etna", "qualname": "FoldersAndFilesResponse.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    folders: List[etna.hooks.etna.Folder] = <factory>,\n    files: List[etna.hooks.etna.File] = <factory>\n)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.FoldersAndFilesResponse.empty", "modulename": "etna.hooks.etna", "qualname": "FoldersAndFilesResponse.empty", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.FoldersAndFilesResponse.extend", "modulename": "etna.hooks.etna", "qualname": "FoldersAndFilesResponse.extend", "type": "function", "doc": "<p></p>\n", "signature": "(self, other: etna.hooks.etna.FoldersAndFilesResponse)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.FoldersResponse", "modulename": "etna.hooks.etna", "qualname": "FoldersResponse", "type": "class", "doc": "<p>FoldersResponse(folders: List[etna.hooks.etna.Folder])</p>\n"}, {"fullname": "etna.hooks.etna.FoldersResponse.__init__", "modulename": "etna.hooks.etna", "qualname": "FoldersResponse.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, folders: List[etna.hooks.etna.Folder])", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Metis", "modulename": "etna.hooks.etna", "qualname": "Metis", "type": "class", "doc": "<p></p>\n", "bases": "EtnaClientBase"}, {"fullname": "etna.hooks.etna.Metis.touch_folder", "modulename": "etna.hooks.etna", "qualname": "Metis.touch_folder", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    project_name: str,\n    bucket_name: str,\n    folder_path: str\n) -> etna.hooks.etna.FoldersResponse", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Metis.list_folder", "modulename": "etna.hooks.etna", "qualname": "Metis.list_folder", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    project_name: str,\n    bucket_name: str,\n    folder_path: Union[str, NoneType] = None\n) -> etna.hooks.etna.FoldersAndFilesResponse", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Metis.authorize_download", "modulename": "etna.hooks.etna", "qualname": "Metis.authorize_download", "type": "function", "doc": "<p></p>\n", "signature": "(self, project_name: str, bucket_name: str, file_path: str) -> str", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Metis.open_file", "modulename": "etna.hooks.etna", "qualname": "Metis.open_file", "type": "function", "doc": "<p>See open_url, this method opens the given file's download_url using that method.\nIf the given file does not have a download_url set, it will fetch one using <code>authorize_download</code>.</p>\n", "signature": "(\n    self,\n    file: etna.hooks.etna.File,\n    binary_mode=False\n) -> _io.BufferedReader", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Metis.open_url", "modulename": "etna.hooks.etna", "qualname": "Metis.open_url", "type": "function", "doc": "<p>Opens the given url for download into a context as a python io (file like) object.\nBy default, when binary_mode is False, the underlying io object yields decoded strings using utf-8.\nOtherwise, the provided io object yields bytes objects.</p>\n\n<p>Note:  Ideally, this method is used in combination with 'with' syntax in python, so that the underlying\ndata stream is closed after usage.  This is especially performant when code only needs to access a small\nsubset of the data.</p>\n\n<p>eg:</p>\n\n<pre><code>with metis.open_url(file.download_url) as open_file:\n  for line in csv.reader(open_file):\n     break\n</code></pre>\n", "signature": "(self, url: str, binary_mode=False) -> _io.BufferedReader", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Metis.authorize_upload", "modulename": "etna.hooks.etna", "qualname": "Metis.authorize_upload", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    project_name: str,\n    bucket_name: str,\n    file_path: str\n) -> etna.hooks.etna.UploadResponse", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Metis.start_upload", "modulename": "etna.hooks.etna", "qualname": "Metis.start_upload", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    request: etna.hooks.etna.UploadStartRequest\n) -> etna.hooks.etna.UploadResponse", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Metis.upload_blob", "modulename": "etna.hooks.etna", "qualname": "Metis.upload_blob", "type": "function", "doc": "<p></p>\n", "signature": "(self, request: etna.hooks.etna.UploadBlobRequest)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Metis.upload_file", "modulename": "etna.hooks.etna", "qualname": "Metis.upload_file", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    project_name: str,\n    bucket_name: str,\n    dest_path: str,\n    file: <class 'IO'>,\n    size: Union[int, NoneType] = None,\n    max_retries=3\n) -> collections.abc.Iterable[etna.hooks.etna.Upload]", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Metis.upload_parts", "modulename": "etna.hooks.etna", "qualname": "Metis.upload_parts", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    upload: etna.hooks.etna.Upload,\n    metis_uid: str,\n    remaining_attempts: int,\n    reset=False\n) -> collections.abc.Iterable[etna.hooks.etna.Upload]", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Metis.create_folder", "modulename": "etna.hooks.etna", "qualname": "Metis.create_folder", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    project_name: str,\n    bucket_name: str,\n    folder_path: str\n) -> etna.hooks.etna.FoldersAndFilesResponse", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Metis.folder_exists", "modulename": "etna.hooks.etna", "qualname": "Metis.folder_exists", "type": "function", "doc": "<p></p>\n", "signature": "(self, project_name: str, bucket_name: str, folder_path: str) -> bool", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Metis.tail", "modulename": "etna.hooks.etna", "qualname": "Metis.tail", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    project_name: str,\n    bucket_name: str,\n    type: Union[Literal['folders'], Literal['files']],\n    batch_start: Union[datetime.datetime, NoneType] = None,\n    batch_end: Union[datetime.datetime, NoneType] = None,\n    folder_id: Union[int, List[int], NoneType] = None\n) -> Tuple[List[etna.hooks.etna.File], List[etna.hooks.etna.Folder]]", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Metis.find", "modulename": "etna.hooks.etna", "qualname": "Metis.find", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    project_name: str,\n    bucket_name: str,\n    params: List[Mapping],\n    limit: Union[int, NoneType] = None,\n    offset: Union[int, NoneType] = None,\n    hide_paths=False\n) -> etna.hooks.etna.FoldersAndFilesResponse", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Attribute", "modulename": "etna.hooks.etna", "qualname": "Attribute", "type": "class", "doc": "<p>Attribute(attribute_type: str = '', link_model_name: Union[str, NoneType] = None, match: Union[str, NoneType] = None, restricted: Union[bool, NoneType] = None, read_only: Union[bool, NoneType] = None, hidden: Union[bool, NoneType] = None, validation: Any = None)</p>\n"}, {"fullname": "etna.hooks.etna.Attribute.__init__", "modulename": "etna.hooks.etna", "qualname": "Attribute.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    attribute_type: str = '',\n    link_model_name: Union[str, NoneType] = None,\n    match: Union[str, NoneType] = None,\n    restricted: Union[bool, NoneType] = None,\n    read_only: Union[bool, NoneType] = None,\n    hidden: Union[bool, NoneType] = None,\n    validation: Any = None\n)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Attribute.attribute_type", "modulename": "etna.hooks.etna", "qualname": "Attribute.attribute_type", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.Attribute.link_model_name", "modulename": "etna.hooks.etna", "qualname": "Attribute.link_model_name", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[str, NoneType]", "default_value": " = None"}, {"fullname": "etna.hooks.etna.Attribute.match", "modulename": "etna.hooks.etna", "qualname": "Attribute.match", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[str, NoneType]", "default_value": " = None"}, {"fullname": "etna.hooks.etna.Attribute.restricted", "modulename": "etna.hooks.etna", "qualname": "Attribute.restricted", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[bool, NoneType]", "default_value": " = None"}, {"fullname": "etna.hooks.etna.Attribute.read_only", "modulename": "etna.hooks.etna", "qualname": "Attribute.read_only", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[bool, NoneType]", "default_value": " = None"}, {"fullname": "etna.hooks.etna.Attribute.hidden", "modulename": "etna.hooks.etna", "qualname": "Attribute.hidden", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[bool, NoneType]", "default_value": " = None"}, {"fullname": "etna.hooks.etna.Attribute.validation", "modulename": "etna.hooks.etna", "qualname": "Attribute.validation", "type": "variable", "doc": "<p></p>\n", "annotation": ": Any", "default_value": " = None"}, {"fullname": "etna.hooks.etna.Template", "modulename": "etna.hooks.etna", "qualname": "Template", "type": "class", "doc": "<p>Template(attributes: Dict[str, etna.hooks.etna.Attribute], name: str = '', identifier: str = '', version: int = 0, parent: str = '')</p>\n"}, {"fullname": "etna.hooks.etna.Template.__init__", "modulename": "etna.hooks.etna", "qualname": "Template.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    attributes: Dict[str, etna.hooks.etna.Attribute],\n    name: str = '',\n    identifier: str = '',\n    version: int = 0,\n    parent: str = ''\n)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Template.name", "modulename": "etna.hooks.etna", "qualname": "Template.name", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.Template.identifier", "modulename": "etna.hooks.etna", "qualname": "Template.identifier", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.Template.version", "modulename": "etna.hooks.etna", "qualname": "Template.version", "type": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": " = 0"}, {"fullname": "etna.hooks.etna.Template.parent", "modulename": "etna.hooks.etna", "qualname": "Template.parent", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.Model", "modulename": "etna.hooks.etna", "qualname": "Model", "type": "class", "doc": "<p>Model(documents: Dict[str, Dict[str, Any]] = <factory>, template: Union[etna.hooks.etna.Template, NoneType] = None, count: int = 0)</p>\n"}, {"fullname": "etna.hooks.etna.Model.__init__", "modulename": "etna.hooks.etna", "qualname": "Model.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    documents: Dict[str, Dict[str, Any]] = <factory>,\n    template: Union[etna.hooks.etna.Template, NoneType] = None,\n    count: int = 0\n)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Model.template", "modulename": "etna.hooks.etna", "qualname": "Model.template", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[etna.hooks.etna.Template, NoneType]", "default_value": " = None"}, {"fullname": "etna.hooks.etna.Model.count", "modulename": "etna.hooks.etna", "qualname": "Model.count", "type": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": " = 0"}, {"fullname": "etna.hooks.etna.Model.empty", "modulename": "etna.hooks.etna", "qualname": "Model.empty", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Model.extend", "modulename": "etna.hooks.etna", "qualname": "Model.extend", "type": "function", "doc": "<p></p>\n", "signature": "(self, other: etna.hooks.etna.Model)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.RetrievalResponse", "modulename": "etna.hooks.etna", "qualname": "RetrievalResponse", "type": "class", "doc": "<p>RetrievalResponse(models: Dict[str, etna.hooks.etna.Model] = <factory>)</p>\n"}, {"fullname": "etna.hooks.etna.RetrievalResponse.__init__", "modulename": "etna.hooks.etna", "qualname": "RetrievalResponse.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, models: Dict[str, etna.hooks.etna.Model] = <factory>)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.RetrievalResponse.empty", "modulename": "etna.hooks.etna", "qualname": "RetrievalResponse.empty", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.RetrievalResponse.extend", "modulename": "etna.hooks.etna", "qualname": "RetrievalResponse.extend", "type": "function", "doc": "<p></p>\n", "signature": "(self, other: etna.hooks.etna.RetrievalResponse)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.UploadResponse", "modulename": "etna.hooks.etna", "qualname": "UploadResponse", "type": "class", "doc": "<p>UploadResponse(current_byte_position: int = 0, url: str = '', next_blob_size: int = 0)</p>\n"}, {"fullname": "etna.hooks.etna.UploadResponse.__init__", "modulename": "etna.hooks.etna", "qualname": "UploadResponse.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    current_byte_position: int = 0,\n    url: str = '',\n    next_blob_size: int = 0\n)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.UploadResponse.current_byte_position", "modulename": "etna.hooks.etna", "qualname": "UploadResponse.current_byte_position", "type": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": " = 0"}, {"fullname": "etna.hooks.etna.UploadResponse.url", "modulename": "etna.hooks.etna", "qualname": "UploadResponse.url", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.UploadResponse.next_blob_size", "modulename": "etna.hooks.etna", "qualname": "UploadResponse.next_blob_size", "type": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": " = 0"}, {"fullname": "etna.hooks.etna.UploadResponse.upload_path", "modulename": "etna.hooks.etna", "qualname": "UploadResponse.upload_path", "type": "variable", "doc": "<p></p>\n"}, {"fullname": "etna.hooks.etna.Upload", "modulename": "etna.hooks.etna", "qualname": "Upload", "type": "class", "doc": "<p>Upload(file: <class 'IO'>, file_size: int, upload_path: str, cur: int = 0, last_bytes: Union[bytes, NoneType] = None, read_position: int = 0)</p>\n"}, {"fullname": "etna.hooks.etna.Upload.__init__", "modulename": "etna.hooks.etna", "qualname": "Upload.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    file: <class 'IO'>,\n    file_size: int,\n    upload_path: str,\n    cur: int = 0,\n    last_bytes: Union[bytes, NoneType] = None,\n    read_position: int = 0\n)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Upload.cur", "modulename": "etna.hooks.etna", "qualname": "Upload.cur", "type": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": " = 0"}, {"fullname": "etna.hooks.etna.Upload.last_bytes", "modulename": "etna.hooks.etna", "qualname": "Upload.last_bytes", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[bytes, NoneType]", "default_value": " = None"}, {"fullname": "etna.hooks.etna.Upload.read_position", "modulename": "etna.hooks.etna", "qualname": "Upload.read_position", "type": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": " = 0"}, {"fullname": "etna.hooks.etna.Upload.INITIAL_BLOB_SIZE", "modulename": "etna.hooks.etna", "qualname": "Upload.INITIAL_BLOB_SIZE", "type": "variable", "doc": "<p></p>\n", "default_value": " = 1024"}, {"fullname": "etna.hooks.etna.Upload.MAX_BLOB_SIZE", "modulename": "etna.hooks.etna", "qualname": "Upload.MAX_BLOB_SIZE", "type": "variable", "doc": "<p></p>\n", "default_value": " = 4194304"}, {"fullname": "etna.hooks.etna.Upload.ZERO_HASH", "modulename": "etna.hooks.etna", "qualname": "Upload.ZERO_HASH", "type": "variable", "doc": "<p></p>\n", "default_value": " = 'd41d8cd98f00b204e9800998ecf8427e'"}, {"fullname": "etna.hooks.etna.Upload.as_magma_file_attribute", "modulename": "etna.hooks.etna", "qualname": "Upload.as_magma_file_attribute", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[etna.hooks.etna.MagmaFileEntry, NoneType]"}, {"fullname": "etna.hooks.etna.Upload.UPLOAD_PATH_REGEX", "modulename": "etna.hooks.etna", "qualname": "Upload.UPLOAD_PATH_REGEX", "type": "variable", "doc": "<p></p>\n", "default_value": " = re.compile('/([^/]+)/upload/([^/]+)/?([^?]*)')"}, {"fullname": "etna.hooks.etna.Upload.as_metis_url", "modulename": "etna.hooks.etna", "qualname": "Upload.as_metis_url", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[str, NoneType]"}, {"fullname": "etna.hooks.etna.Upload.as_file", "modulename": "etna.hooks.etna", "qualname": "Upload.as_file", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[etna.hooks.etna.File, NoneType]"}, {"fullname": "etna.hooks.etna.Upload.next_blob_size", "modulename": "etna.hooks.etna", "qualname": "Upload.next_blob_size", "type": "variable", "doc": "<p></p>\n"}, {"fullname": "etna.hooks.etna.Upload.is_complete", "modulename": "etna.hooks.etna", "qualname": "Upload.is_complete", "type": "variable", "doc": "<p></p>\n"}, {"fullname": "etna.hooks.etna.Upload.next_blob_hash", "modulename": "etna.hooks.etna", "qualname": "Upload.next_blob_hash", "type": "variable", "doc": "<p></p>\n"}, {"fullname": "etna.hooks.etna.Upload.resume_from", "modulename": "etna.hooks.etna", "qualname": "Upload.resume_from", "type": "function", "doc": "<p></p>\n", "signature": "(self, upload_response: etna.hooks.etna.UploadResponse)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Upload.advance_position", "modulename": "etna.hooks.etna", "qualname": "Upload.advance_position", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Upload.read_next_blob", "modulename": "etna.hooks.etna", "qualname": "Upload.read_next_blob", "type": "function", "doc": "<p></p>\n", "signature": "(self) -> bytes", "funcdef": "def"}, {"fullname": "etna.hooks.etna.UploadStartRequest", "modulename": "etna.hooks.etna", "qualname": "UploadStartRequest", "type": "class", "doc": "<p>UploadStartRequest(file_size: int = 0, action: str = 'start', metis_uid: str = '', next_blob_size: int = 0, upload_path: str = '', next_blob_hash: str = '', reset: bool = False)</p>\n"}, {"fullname": "etna.hooks.etna.UploadStartRequest.__init__", "modulename": "etna.hooks.etna", "qualname": "UploadStartRequest.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    file_size: int = 0,\n    action: str = 'start',\n    metis_uid: str = '',\n    next_blob_size: int = 0,\n    upload_path: str = '',\n    next_blob_hash: str = '',\n    reset: bool = False\n)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.UploadStartRequest.file_size", "modulename": "etna.hooks.etna", "qualname": "UploadStartRequest.file_size", "type": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": " = 0"}, {"fullname": "etna.hooks.etna.UploadStartRequest.action", "modulename": "etna.hooks.etna", "qualname": "UploadStartRequest.action", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = 'start'"}, {"fullname": "etna.hooks.etna.UploadStartRequest.metis_uid", "modulename": "etna.hooks.etna", "qualname": "UploadStartRequest.metis_uid", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.UploadStartRequest.next_blob_size", "modulename": "etna.hooks.etna", "qualname": "UploadStartRequest.next_blob_size", "type": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": " = 0"}, {"fullname": "etna.hooks.etna.UploadStartRequest.upload_path", "modulename": "etna.hooks.etna", "qualname": "UploadStartRequest.upload_path", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.UploadStartRequest.next_blob_hash", "modulename": "etna.hooks.etna", "qualname": "UploadStartRequest.next_blob_hash", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.UploadStartRequest.reset", "modulename": "etna.hooks.etna", "qualname": "UploadStartRequest.reset", "type": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": " = False"}, {"fullname": "etna.hooks.etna.UploadBlobRequest", "modulename": "etna.hooks.etna", "qualname": "UploadBlobRequest", "type": "class", "doc": "<p>UploadBlobRequest(file_size: int = 0, action: str = 'blob', metis_uid: str = '', blob_data: bytes = b'', upload_path: str = '', next_blob_size: int = 0, next_blob_hash: str = '', current_byte_position: int = 0)</p>\n"}, {"fullname": "etna.hooks.etna.UploadBlobRequest.__init__", "modulename": "etna.hooks.etna", "qualname": "UploadBlobRequest.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    file_size: int = 0,\n    action: str = 'blob',\n    metis_uid: str = '',\n    blob_data: bytes = b'',\n    upload_path: str = '',\n    next_blob_size: int = 0,\n    next_blob_hash: str = '',\n    current_byte_position: int = 0\n)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.UploadBlobRequest.file_size", "modulename": "etna.hooks.etna", "qualname": "UploadBlobRequest.file_size", "type": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": " = 0"}, {"fullname": "etna.hooks.etna.UploadBlobRequest.action", "modulename": "etna.hooks.etna", "qualname": "UploadBlobRequest.action", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = 'blob'"}, {"fullname": "etna.hooks.etna.UploadBlobRequest.metis_uid", "modulename": "etna.hooks.etna", "qualname": "UploadBlobRequest.metis_uid", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.UploadBlobRequest.blob_data", "modulename": "etna.hooks.etna", "qualname": "UploadBlobRequest.blob_data", "type": "variable", "doc": "<p></p>\n", "annotation": ": bytes", "default_value": " = b''"}, {"fullname": "etna.hooks.etna.UploadBlobRequest.upload_path", "modulename": "etna.hooks.etna", "qualname": "UploadBlobRequest.upload_path", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.UploadBlobRequest.next_blob_size", "modulename": "etna.hooks.etna", "qualname": "UploadBlobRequest.next_blob_size", "type": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": " = 0"}, {"fullname": "etna.hooks.etna.UploadBlobRequest.next_blob_hash", "modulename": "etna.hooks.etna", "qualname": "UploadBlobRequest.next_blob_hash", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.UploadBlobRequest.current_byte_position", "modulename": "etna.hooks.etna", "qualname": "UploadBlobRequest.current_byte_position", "type": "variable", "doc": "<p></p>\n", "annotation": ": int", "default_value": " = 0"}, {"fullname": "etna.hooks.etna.UploadBlobRequest.to_dict", "modulename": "etna.hooks.etna", "qualname": "UploadBlobRequest.to_dict", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.UpdateRequest", "modulename": "etna.hooks.etna", "qualname": "UpdateRequest", "type": "class", "doc": "<p>UpdateRequest(revisions: Dict[str, Dict[str, Dict[str, Any]]] = <factory>, project_name: str = '', dry_run: bool = False)</p>\n"}, {"fullname": "etna.hooks.etna.UpdateRequest.__init__", "modulename": "etna.hooks.etna", "qualname": "UpdateRequest.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    revisions: Dict[str, Dict[str, Dict[str, Any]]] = <factory>,\n    project_name: str = '',\n    dry_run: bool = False\n)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.UpdateRequest.project_name", "modulename": "etna.hooks.etna", "qualname": "UpdateRequest.project_name", "type": "variable", "doc": "<p></p>\n", "annotation": ": str", "default_value": " = ''"}, {"fullname": "etna.hooks.etna.UpdateRequest.dry_run", "modulename": "etna.hooks.etna", "qualname": "UpdateRequest.dry_run", "type": "variable", "doc": "<p></p>\n", "annotation": ": bool", "default_value": " = False"}, {"fullname": "etna.hooks.etna.UpdateRequest.shallow_copy", "modulename": "etna.hooks.etna", "qualname": "UpdateRequest.shallow_copy", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    revisions: Union[Dict[str, Dict[str, Dict[str, Any]]], NoneType] = None\n) -> etna.hooks.etna.UpdateRequest", "funcdef": "def"}, {"fullname": "etna.hooks.etna.UpdateRequest.includes", "modulename": "etna.hooks.etna", "qualname": "UpdateRequest.includes", "type": "function", "doc": "<p></p>\n", "signature": "(self, model_name: str, record_name: str)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.UpdateRequest.sample_revision_tree", "modulename": "etna.hooks.etna", "qualname": "UpdateRequest.sample_revision_tree", "type": "function", "doc": "<p>Attempts to sample the complete graph of related objects reference by the given model_name / record_name in this\nupdate, copying it and all its referenced links to acc.</p>\n\n<p>When destructive=True is set, it also removes all copied models from this update.</p>\n", "signature": "(\n    self,\n    model_name: str,\n    record_name: str,\n    models: Dict[str, etna.hooks.etna.Model],\n    acc: etna.hooks.etna.UpdateRequest,\n    destructive=False\n)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.UpdateRequest.empty", "modulename": "etna.hooks.etna", "qualname": "UpdateRequest.empty", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.UpdateRequest.extend", "modulename": "etna.hooks.etna", "qualname": "UpdateRequest.extend", "type": "function", "doc": "<p></p>\n", "signature": "(self, other: etna.hooks.etna.UpdateRequest)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.UpdateRequest.validate", "modulename": "etna.hooks.etna", "qualname": "UpdateRequest.validate", "type": "function", "doc": "<p></p>\n", "signature": "(self, models: Dict[str, etna.hooks.etna.Model]) -> Iterable[str]", "funcdef": "def"}, {"fullname": "etna.hooks.etna.UpdateRequest.update_record", "modulename": "etna.hooks.etna", "qualname": "UpdateRequest.update_record", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    model_name: str,\n    record_name: str,\n    attrs: Dict[str, Any] = {}\n) -> Dict[str, Any]", "funcdef": "def"}, {"fullname": "etna.hooks.etna.UpdateRequest.append_table", "modulename": "etna.hooks.etna", "qualname": "UpdateRequest.append_table", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    parent_model_name: str,\n    parent_record_name: str,\n    model_name: str,\n    attrs: Dict[str, Any],\n    attribute_name: str\n)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Magma", "modulename": "etna.hooks.etna", "qualname": "Magma", "type": "class", "doc": "<p></p>\n", "bases": "EtnaClientBase"}, {"fullname": "etna.hooks.etna.Magma.retrieve", "modulename": "etna.hooks.etna", "qualname": "Magma.retrieve", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    project_name: str,\n    model_name='all',\n    attribute_names='all',\n    record_names: Union[List[str], <built-in function all>] = [],\n    page: Union[int, NoneType] = None,\n    page_size: Union[int, NoneType] = None,\n    order: Union[str, NoneType] = None,\n    filter: Union[str, NoneType] = None,\n    hide_templates=True,\n    show_disconnected=False\n) -> etna.hooks.etna.RetrievalResponse", "funcdef": "def"}, {"fullname": "etna.hooks.etna.Magma.batch_size", "modulename": "etna.hooks.etna", "qualname": "Magma.batch_size", "type": "variable", "doc": "<p></p>\n", "default_value": " = 300"}, {"fullname": "etna.hooks.etna.Magma.update", "modulename": "etna.hooks.etna", "qualname": "Magma.update", "type": "function", "doc": "<p></p>\n", "signature": "(self, update: etna.hooks.etna.UpdateRequest)", "funcdef": "def"}, {"fullname": "etna.hooks.etna.normalize_for_magma", "modulename": "etna.hooks.etna", "qualname": "normalize_for_magma", "type": "function", "doc": "<p></p>\n", "signature": "(value)", "funcdef": "def"}, {"fullname": "etna.hooks.git", "modulename": "etna.hooks.git", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.hooks.git.GitHook", "modulename": "etna.hooks.git", "qualname": "GitHook", "type": "class", "doc": "<p>Hook to connect to a remote git repository using an ssh key.</p>\n", "bases": "airflow.hooks.base.BaseHook"}, {"fullname": "etna.hooks.git.GitHook.__init__", "modulename": "etna.hooks.git", "qualname": "GitHook.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    git_conn_id: str,\n    remote_path: str,\n    local_path: Union[str, NoneType] = None\n)", "funcdef": "def"}, {"fullname": "etna.hooks.git.GitHook.conn_name_attr", "modulename": "etna.hooks.git", "qualname": "GitHook.conn_name_attr", "type": "variable", "doc": "<p></p>\n", "default_value": " = 'git_conn_id'"}, {"fullname": "etna.hooks.git.GitHook.default_conn_name", "modulename": "etna.hooks.git", "qualname": "GitHook.default_conn_name", "type": "variable", "doc": "<p></p>\n", "default_value": " = 'git_default'"}, {"fullname": "etna.hooks.git.GitHook.conn_type", "modulename": "etna.hooks.git", "qualname": "GitHook.conn_type", "type": "variable", "doc": "<p></p>\n", "default_value": " = 'git'"}, {"fullname": "etna.hooks.git.GitHook.hook_name", "modulename": "etna.hooks.git", "qualname": "GitHook.hook_name", "type": "variable", "doc": "<p></p>\n", "default_value": " = 'Git Checkout'"}, {"fullname": "etna.hooks.git.GitHook.root_dir", "modulename": "etna.hooks.git", "qualname": "GitHook.root_dir", "type": "variable", "doc": "<p></p>\n", "default_value": " = '/opt/airflow/dags/repos'"}, {"fullname": "etna.hooks.git.GitHook.get_ui_field_behaviour", "modulename": "etna.hooks.git", "qualname": "GitHook.get_ui_field_behaviour", "type": "function", "doc": "<p></p>\n", "signature": "() -> Dict", "funcdef": "def"}, {"fullname": "etna.hooks.git.GitHook.get_connection_form_widgets", "modulename": "etna.hooks.git", "qualname": "GitHook.get_connection_form_widgets", "type": "function", "doc": "<p>Returns dictionary of widgets to be added for the hook to handle extra values.</p>\n\n<p>If you have class hierarchy, usually the widgets needed by your class are already\nadded by the base class, so there is no need to implement this method. It might\nactually result in warning in the logs if you try to add widgets that have already\nbeen added by the base class.</p>\n\n<p>Note that values of Dict should be of wtforms.Field type. It's not added here\nfor the efficiency of imports.</p>\n", "signature": "() -> Dict", "funcdef": "def"}, {"fullname": "etna.hooks.git.GitHook.key_path", "modulename": "etna.hooks.git", "qualname": "GitHook.key_path", "type": "function", "doc": "<p></p>\n", "signature": "(self) -> Union[str, NoneType]", "funcdef": "def"}, {"fullname": "etna.hooks.git.GitHook.get_conn", "modulename": "etna.hooks.git", "qualname": "GitHook.get_conn", "type": "function", "doc": "<p>Returns connection for the hook.</p>\n", "signature": "(self) -> airflow.models.connection.Connection", "funcdef": "def"}, {"fullname": "etna.hooks.git.GitHook.connection", "modulename": "etna.hooks.git", "qualname": "GitHook.connection", "type": "function", "doc": "<p></p>\n", "signature": "(unknown)", "funcdef": "def"}, {"fullname": "etna.hooks.git.GitHook.run_git", "modulename": "etna.hooks.git", "qualname": "GitHook.run_git", "type": "function", "doc": "<p></p>\n", "signature": "(self, working_dir: str, *command: str)", "funcdef": "def"}, {"fullname": "etna.hooks.git.GitHook.clone", "modulename": "etna.hooks.git", "qualname": "GitHook.clone", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "etna.hooks.git.GitHook.pull", "modulename": "etna.hooks.git", "qualname": "GitHook.pull", "type": "function", "doc": "<p></p>\n", "signature": "(self, *args: str)", "funcdef": "def"}, {"fullname": "etna.hooks.git.GitHook.checkout", "modulename": "etna.hooks.git", "qualname": "GitHook.checkout", "type": "function", "doc": "<p></p>\n", "signature": "(self, *args: str)", "funcdef": "def"}, {"fullname": "etna.hooks.git.GitHook.commit", "modulename": "etna.hooks.git", "qualname": "GitHook.commit", "type": "function", "doc": "<p></p>\n", "signature": "(self, *args: str)", "funcdef": "def"}, {"fullname": "etna.hooks.git.GitHook.push", "modulename": "etna.hooks.git", "qualname": "GitHook.push", "type": "function", "doc": "<p></p>\n", "signature": "(self, *args: str)", "funcdef": "def"}, {"fullname": "etna.hooks.hook_helpers", "modulename": "etna.hooks.hook_helpers", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.hooks.hook_helpers.get_project_slack_hook", "modulename": "etna.hooks.hook_helpers", "qualname": "get_project_slack_hook", "type": "function", "doc": "<p></p>\n", "signature": "() -> airflow.providers.slack.hooks.slack.SlackHook", "funcdef": "def"}, {"fullname": "etna.hooks.hook_helpers.get_git_hook", "modulename": "etna.hooks.hook_helpers", "qualname": "get_git_hook", "type": "function", "doc": "<p></p>\n", "signature": "(remote_path: str) -> etna.hooks.git.GitHook", "funcdef": "def"}, {"fullname": "etna.hooks.hook_helpers.get_etna_hook", "modulename": "etna.hooks.hook_helpers", "qualname": "get_etna_hook", "type": "function", "doc": "<p></p>\n", "signature": "() -> etna.hooks.etna.EtnaHook", "funcdef": "def"}, {"fullname": "etna.hooks.keys", "modulename": "etna.hooks.keys", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.hooks.keys.prepared_key_from", "modulename": "etna.hooks.keys", "qualname": "prepared_key_from", "type": "function", "doc": "<p>Attempts to create an ssh compatible key file from the password field of a connection.\nUse this with a context eg:\n    with prepared_key_from(conn) as key_file_path:\n      # only use the key file path here.</p>\n\n<p>NOTE: The file that backs the key will be deleted once the context is left, so be mindful.\nThe file will only be readable by this process user.</p>\n", "signature": "(\n    connection: airflow.models.connection.Connection,\n    assert_password_non_empty=False\n) -> AbstractContextManager[Union[str, NoneType]]", "funcdef": "def"}, {"fullname": "etna.operators", "modulename": "etna.operators", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.operators.run_on_docker", "modulename": "etna.operators", "qualname": "run_on_docker", "type": "function", "doc": "<p></p>\n", "signature": "(\n    task_id: str,\n    source_service: str,\n    command: List[str],\n    env: Mapping[str, str] = {},\n    output_json: Union[bool, Type] = False,\n    output_b64: bool = False,\n    docker_base_url='unix://var/run/docker.sock',\n    include_external_networks=False\n) -> etna.operators.docker_operator_base.DockerOperatorBase", "funcdef": "def"}, {"fullname": "etna.operators.run_in_swarm", "modulename": "etna.operators", "qualname": "run_in_swarm", "type": "function", "doc": "<p></p>\n", "signature": "(\n    task_id: str,\n    source_service: str,\n    command: List[str],\n    env: Mapping[str, str] = {},\n    output_json: Union[bool, Type] = False,\n    output_b64: bool = False,\n    docker_base_url='unix://var/run/docker.sock',\n    include_external_networks: Union[bool, NoneType] = False\n) -> etna.operators.swarm_operator.DockerSwarmOperator", "funcdef": "def"}, {"fullname": "etna.operators.run_in_container", "modulename": "etna.operators", "qualname": "run_in_container", "type": "function", "doc": "<p></p>\n", "signature": "(\n    task_id: str,\n    source_container: str,\n    command: List[str],\n    env: Mapping[str, str] = {},\n    use_compose: bool = True,\n    output_json: Union[bool, Type] = False,\n    output_b64: bool = False,\n    docker_base_url='unix://var/run/docker.sock'\n) -> etna.operators.docker_operator.DockerOperator", "funcdef": "def"}, {"fullname": "etna.operators.docker_operator", "modulename": "etna.operators.docker_operator", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.operators.docker_operator.find_container", "modulename": "etna.operators.docker_operator", "qualname": "find_container", "type": "function", "doc": "<p></p>\n", "signature": "(\n    cli: docker.api.client.APIClient,\n    container_name: str\n) -> Union[docker.models.containers.Container, NoneType]", "funcdef": "def"}, {"fullname": "etna.operators.docker_operator.DockerOperator", "modulename": "etna.operators.docker_operator", "qualname": "DockerOperator", "type": "class", "doc": "<p>Abstract base class for all operators. Since operators create objects that\nbecome nodes in the dag, BaseOperator contains many recursive methods for\ndag crawling behavior. To derive this class, you are expected to override\nthe constructor as well as the 'execute' method.</p>\n\n<p>Operators derived from this class should perform or trigger certain tasks\nsynchronously (wait for completion). Example of operators could be an\noperator that runs a Pig job (PigOperator), a sensor operator that\nwaits for a partition to land in Hive (HiveSensorOperator), or one that\nmoves data from Hive to MySQL (Hive2MySqlOperator). Instances of these\noperators (tasks) target specific operations, running specific scripts,\nfunctions or data transfers.</p>\n\n<p>This class is abstract and shouldn't be instantiated. Instantiating a\nclass derived from this one results in the creation of a task object,\nwhich ultimately becomes a node in DAG objects. Task dependencies should\nbe set by using the set_upstream and/or set_downstream methods.</p>\n\n<p>:param task_id: a unique, meaningful id for the task\n:type task_id: str\n:param owner: the owner of the task. Using a meaningful description\n    (e.g. user/person/team/role name) to clarify ownership is recommended.\n:type owner: str\n:param email: the 'to' email address(es) used in email alerts. This can be a\n    single email or multiple ones. Multiple addresses can be specified as a\n    comma or semi-colon separated string or by passing a list of strings.\n:type email: str or list[str]\n:param email_on_retry: Indicates whether email alerts should be sent when a\n    task is retried\n:type email_on_retry: bool\n:param email_on_failure: Indicates whether email alerts should be sent when\n    a task failed\n:type email_on_failure: bool\n:param retries: the number of retries that should be performed before\n    failing the task\n:type retries: int\n:param retry_delay: delay between retries\n:type retry_delay: datetime.timedelta\n:param retry_exponential_backoff: allow progressive longer waits between\n    retries by using exponential backoff algorithm on retry delay (delay\n    will be converted into seconds)\n:type retry_exponential_backoff: bool\n:param max_retry_delay: maximum delay interval between retries\n:type max_retry_delay: datetime.timedelta\n:param start_date: The <code>start_date</code> for the task, determines\n    the <code>execution_date</code> for the first task instance. The best practice\n    is to have the start_date rounded\n    to your DAG's <code>schedule_interval</code>. Daily jobs have their start_date\n    some day at 00:00:00, hourly jobs have their start_date at 00:00\n    of a specific hour. Note that Airflow simply looks at the latest\n    <code>execution_date</code> and adds the <code>schedule_interval</code> to determine\n    the next <code>execution_date</code>. It is also very important\n    to note that different tasks' dependencies\n    need to line up in time. If task A depends on task B and their\n    start_date are offset in a way that their execution_date don't line\n    up, A's dependencies will never be met. If you are looking to delay\n    a task, for example running a daily task at 2AM, look into the\n    <code>TimeSensor</code> and <code>TimeDeltaSensor</code>. We advise against using\n    dynamic <code>start_date</code> and recommend using fixed ones. Read the\n    FAQ entry about start_date for more information.\n:type start_date: datetime.datetime\n:param end_date: if specified, the scheduler won't go beyond this date\n:type end_date: datetime.datetime\n:param depends_on_past: when set to true, task instances will run\n    sequentially and only if the previous instance has succeeded or has been skipped.\n    The task instance for the start_date is allowed to run.\n:type depends_on_past: bool\n:param wait_for_downstream: when set to true, an instance of task\n    X will wait for tasks immediately downstream of the previous instance\n    of task X to finish successfully or be skipped before it runs. This is useful if the\n    different instances of a task X alter the same asset, and this asset\n    is used by tasks downstream of task X. Note that depends_on_past\n    is forced to True wherever wait_for_downstream is used. Also note that\n    only tasks <em>immediately</em> downstream of the previous task instance are waited\n    for; the statuses of any tasks further downstream are ignored.\n:type wait_for_downstream: bool\n:param dag: a reference to the dag the task is attached to (if any)\n:type dag: airflow.models.DAG\n:param priority_weight: priority weight of this task against other task.\n    This allows the executor to trigger higher priority tasks before\n    others when things get backed up. Set priority_weight as a higher\n    number for more important tasks.\n:type priority_weight: int\n:param weight_rule: weighting method used for the effective total\n    priority weight of the task. Options are:\n    <code>{ downstream | upstream | absolute }</code> default is <code>downstream</code>\n    When set to <code>downstream</code> the effective weight of the task is the\n    aggregate sum of all downstream descendants. As a result, upstream\n    tasks will have higher weight and will be scheduled more aggressively\n    when using positive weight values. This is useful when you have\n    multiple dag run instances and desire to have all upstream tasks to\n    complete for all runs before each dag can continue processing\n    downstream tasks. When set to <code>upstream</code> the effective weight is the\n    aggregate sum of all upstream ancestors. This is the opposite where\n    downstream tasks have higher weight and will be scheduled more\n    aggressively when using positive weight values. This is useful when you\n    have multiple dag run instances and prefer to have each dag complete\n    before starting upstream tasks of other dags.  When set to\n    <code>absolute</code>, the effective weight is the exact <code>priority_weight</code>\n    specified without additional weighting. You may want to do this when\n    you know exactly what priority weight each task should have.\n    Additionally, when set to <code>absolute</code>, there is bonus effect of\n    significantly speeding up the task creation process as for very large\n    DAGs. Options can be set as string or using the constants defined in\n    the static class <code>airflow.utils.WeightRule</code>\n:type weight_rule: str\n:param queue: which queue to target when running this job. Not\n    all executors implement queue management, the CeleryExecutor\n    does support targeting specific queues.\n:type queue: str\n:param pool: the slot pool this task should run in, slot pools are a\n    way to limit concurrency for certain tasks\n:type pool: str\n:param pool_slots: the number of pool slots this task should use (&gt;= 1)\n    Values less than 1 are not allowed.\n:type pool_slots: int\n:param sla: time by which the job is expected to succeed. Note that\n    this represents the <code>timedelta</code> after the period is closed. For\n    example if you set an SLA of 1 hour, the scheduler would send an email\n    soon after 1:00AM on the <code>2016-01-02</code> if the <code>2016-01-01</code> instance\n    has not succeeded yet.\n    The scheduler pays special attention for jobs with an SLA and\n    sends alert\n    emails for SLA misses. SLA misses are also recorded in the database\n    for future reference. All tasks that share the same SLA time\n    get bundled in a single email, sent soon after that time. SLA\n    notification are sent once and only once for each task instance.\n:type sla: datetime.timedelta\n:param execution_timeout: max time allowed for the execution of\n    this task instance, if it goes beyond it will raise and fail.\n:type execution_timeout: datetime.timedelta\n:param on_failure_callback: a function to be called when a task instance\n    of this task fails. a context dictionary is passed as a single\n    parameter to this function. Context contains references to related\n    objects to the task instance and is documented under the macros\n    section of the API.\n:type on_failure_callback: TaskStateChangeCallback\n:param on_execute_callback: much like the <code>on_failure_callback</code> except\n    that it is executed right before the task is executed.\n:type on_execute_callback: TaskStateChangeCallback\n:param on_retry_callback: much like the <code>on_failure_callback</code> except\n    that it is executed when retries occur.\n:type on_retry_callback: TaskStateChangeCallback\n:param on_success_callback: much like the <code>on_failure_callback</code> except\n    that it is executed when the task succeeds.\n:type on_success_callback: TaskStateChangeCallback\n:param pre_execute: a function to be called immediately before task\n    execution, receiving a context dictionary; raising an exception will\n    prevent the task from being executed.</p>\n\n<pre><code>|experimental|\n</code></pre>\n\n<p>:type pre_execute: TaskPreExecuteHook\n:param post_execute: a function to be called immediately after task\n    execution, receiving a context dictionary and task result; raising an\n    exception will prevent the task from succeeding.</p>\n\n<pre><code>|experimental|\n</code></pre>\n\n<p>:type post_execute: TaskPostExecuteHook\n:param trigger_rule: defines the rule by which dependencies are applied\n    for the task to get triggered. Options are:\n    <code>{ all_success | all_failed | all_done | one_success |\n    one_failed | none_failed | none_failed_min_one_success | none_skipped | always}</code>\n    default is <code>all_success</code>. Options can be set as string or\n    using the constants defined in the static class\n    <code>airflow.utils.TriggerRule</code>\n:type trigger_rule: str\n:param resources: A map of resource parameter names (the argument names of the\n    Resources constructor) to their values.\n:type resources: dict\n:param run_as_user: unix username to impersonate while running the task\n:type run_as_user: str\n:param max_active_tis_per_dag: When set, a task will be able to limit the concurrent\n    runs across execution_dates.\n:type max_active_tis_per_dag: int\n:param executor_config: Additional task-level configuration parameters that are\n    interpreted by a specific executor. Parameters are namespaced by the name of\n    executor.</p>\n\n<pre><code>**Example**: to run this task in a specific docker container through\nthe KubernetesExecutor ::\n\n    MyOperator(...,\n        executor_config={\n            \"KubernetesExecutor\":\n                {\"image\": \"myCustomDockerImage\"}\n        }\n    )\n</code></pre>\n\n<p>:type executor_config: dict\n:param do_xcom_push: if True, an XCom is pushed containing the Operator's\n    result\n:type do_xcom_push: bool\n:param task_group: The TaskGroup to which the task should belong. This is typically provided when not\n    using a TaskGroup as a context manager.\n:type task_group: airflow.utils.task_group.TaskGroup\n:param doc: Add documentation or notes to your Task objects that is visible in\n    Task Instance details View in the Webserver\n:type doc: str\n:param doc_md: Add documentation (in Markdown format) or notes to your Task objects\n    that is visible in Task Instance details View in the Webserver\n:type doc_md: str\n:param doc_rst: Add documentation (in RST format) or notes to your Task objects\n    that is visible in Task Instance details View in the Webserver\n:type doc_rst: str\n:param doc_json: Add documentation (in JSON format) or notes to your Task objects\n    that is visible in Task Instance details View in the Webserver\n:type doc_json: str\n:param doc_yaml: Add documentation (in YAML format) or notes to your Task objects\n    that is visible in Task Instance details View in the Webserver\n:type doc_yaml: str</p>\n", "bases": "etna.operators.docker_operator_base.DockerOperatorBase"}, {"fullname": "etna.operators.docker_operator.DockerOperator.__init__", "modulename": "etna.operators.docker_operator", "qualname": "DockerOperator.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, *args, source_container_name: str, **kwds)", "funcdef": "def"}, {"fullname": "etna.operators.docker_operator.DockerOperator.pre_execute", "modulename": "etna.operators.docker_operator", "qualname": "DockerOperator.pre_execute", "type": "function", "doc": "<p>This hook is triggered right before self.execute() is called.</p>\n", "signature": "(self, context) -> None", "funcdef": "def"}, {"fullname": "etna.operators.docker_operator.DockerOperator.container_labeling", "modulename": "etna.operators.docker_operator", "qualname": "DockerOperator.container_labeling", "type": "function", "doc": "<p></p>\n", "signature": "()", "funcdef": "def"}, {"fullname": "etna.operators.docker_operator.DockerOperator.cleanup", "modulename": "etna.operators.docker_operator", "qualname": "DockerOperator.cleanup", "type": "function", "doc": "<p></p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "etna.operators.docker_operator.DockerOperator.successful_states", "modulename": "etna.operators.docker_operator", "qualname": "DockerOperator.successful_states", "type": "variable", "doc": "<p></p>\n", "annotation": ": Set[str]"}, {"fullname": "etna.operators.docker_operator.DockerOperator.completed_states", "modulename": "etna.operators.docker_operator", "qualname": "DockerOperator.completed_states", "type": "variable", "doc": "<p></p>\n", "annotation": ": Set[str]"}, {"fullname": "etna.operators.docker_operator.hacky_container_logs", "modulename": "etna.operators.docker_operator", "qualname": "hacky_container_logs", "type": "function", "doc": "<p></p>\n", "signature": "(\n    cli: docker.api.client.APIClient,\n    container: str,\n    since: int\n) -> bytes", "funcdef": "def"}, {"fullname": "etna.operators.docker_operator_base", "modulename": "etna.operators.docker_operator_base", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.operators.docker_operator_base.SwarmSharedData", "modulename": "etna.operators.docker_operator_base", "qualname": "SwarmSharedData", "type": "class", "doc": "<p>SwarmSharedData(data: bytes, remote_path: str)</p>\n"}, {"fullname": "etna.operators.docker_operator_base.SwarmSharedData.__init__", "modulename": "etna.operators.docker_operator_base", "qualname": "SwarmSharedData.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(self, data: bytes, remote_path: str)", "funcdef": "def"}, {"fullname": "etna.operators.docker_operator_base.DockerOperatorBase", "modulename": "etna.operators.docker_operator_base", "qualname": "DockerOperatorBase", "type": "class", "doc": "<p>Abstract base class for all operators. Since operators create objects that\nbecome nodes in the dag, BaseOperator contains many recursive methods for\ndag crawling behavior. To derive this class, you are expected to override\nthe constructor as well as the 'execute' method.</p>\n\n<p>Operators derived from this class should perform or trigger certain tasks\nsynchronously (wait for completion). Example of operators could be an\noperator that runs a Pig job (PigOperator), a sensor operator that\nwaits for a partition to land in Hive (HiveSensorOperator), or one that\nmoves data from Hive to MySQL (Hive2MySqlOperator). Instances of these\noperators (tasks) target specific operations, running specific scripts,\nfunctions or data transfers.</p>\n\n<p>This class is abstract and shouldn't be instantiated. Instantiating a\nclass derived from this one results in the creation of a task object,\nwhich ultimately becomes a node in DAG objects. Task dependencies should\nbe set by using the set_upstream and/or set_downstream methods.</p>\n\n<p>:param task_id: a unique, meaningful id for the task\n:type task_id: str\n:param owner: the owner of the task. Using a meaningful description\n    (e.g. user/person/team/role name) to clarify ownership is recommended.\n:type owner: str\n:param email: the 'to' email address(es) used in email alerts. This can be a\n    single email or multiple ones. Multiple addresses can be specified as a\n    comma or semi-colon separated string or by passing a list of strings.\n:type email: str or list[str]\n:param email_on_retry: Indicates whether email alerts should be sent when a\n    task is retried\n:type email_on_retry: bool\n:param email_on_failure: Indicates whether email alerts should be sent when\n    a task failed\n:type email_on_failure: bool\n:param retries: the number of retries that should be performed before\n    failing the task\n:type retries: int\n:param retry_delay: delay between retries\n:type retry_delay: datetime.timedelta\n:param retry_exponential_backoff: allow progressive longer waits between\n    retries by using exponential backoff algorithm on retry delay (delay\n    will be converted into seconds)\n:type retry_exponential_backoff: bool\n:param max_retry_delay: maximum delay interval between retries\n:type max_retry_delay: datetime.timedelta\n:param start_date: The <code>start_date</code> for the task, determines\n    the <code>execution_date</code> for the first task instance. The best practice\n    is to have the start_date rounded\n    to your DAG's <code>schedule_interval</code>. Daily jobs have their start_date\n    some day at 00:00:00, hourly jobs have their start_date at 00:00\n    of a specific hour. Note that Airflow simply looks at the latest\n    <code>execution_date</code> and adds the <code>schedule_interval</code> to determine\n    the next <code>execution_date</code>. It is also very important\n    to note that different tasks' dependencies\n    need to line up in time. If task A depends on task B and their\n    start_date are offset in a way that their execution_date don't line\n    up, A's dependencies will never be met. If you are looking to delay\n    a task, for example running a daily task at 2AM, look into the\n    <code>TimeSensor</code> and <code>TimeDeltaSensor</code>. We advise against using\n    dynamic <code>start_date</code> and recommend using fixed ones. Read the\n    FAQ entry about start_date for more information.\n:type start_date: datetime.datetime\n:param end_date: if specified, the scheduler won't go beyond this date\n:type end_date: datetime.datetime\n:param depends_on_past: when set to true, task instances will run\n    sequentially and only if the previous instance has succeeded or has been skipped.\n    The task instance for the start_date is allowed to run.\n:type depends_on_past: bool\n:param wait_for_downstream: when set to true, an instance of task\n    X will wait for tasks immediately downstream of the previous instance\n    of task X to finish successfully or be skipped before it runs. This is useful if the\n    different instances of a task X alter the same asset, and this asset\n    is used by tasks downstream of task X. Note that depends_on_past\n    is forced to True wherever wait_for_downstream is used. Also note that\n    only tasks <em>immediately</em> downstream of the previous task instance are waited\n    for; the statuses of any tasks further downstream are ignored.\n:type wait_for_downstream: bool\n:param dag: a reference to the dag the task is attached to (if any)\n:type dag: airflow.models.DAG\n:param priority_weight: priority weight of this task against other task.\n    This allows the executor to trigger higher priority tasks before\n    others when things get backed up. Set priority_weight as a higher\n    number for more important tasks.\n:type priority_weight: int\n:param weight_rule: weighting method used for the effective total\n    priority weight of the task. Options are:\n    <code>{ downstream | upstream | absolute }</code> default is <code>downstream</code>\n    When set to <code>downstream</code> the effective weight of the task is the\n    aggregate sum of all downstream descendants. As a result, upstream\n    tasks will have higher weight and will be scheduled more aggressively\n    when using positive weight values. This is useful when you have\n    multiple dag run instances and desire to have all upstream tasks to\n    complete for all runs before each dag can continue processing\n    downstream tasks. When set to <code>upstream</code> the effective weight is the\n    aggregate sum of all upstream ancestors. This is the opposite where\n    downstream tasks have higher weight and will be scheduled more\n    aggressively when using positive weight values. This is useful when you\n    have multiple dag run instances and prefer to have each dag complete\n    before starting upstream tasks of other dags.  When set to\n    <code>absolute</code>, the effective weight is the exact <code>priority_weight</code>\n    specified without additional weighting. You may want to do this when\n    you know exactly what priority weight each task should have.\n    Additionally, when set to <code>absolute</code>, there is bonus effect of\n    significantly speeding up the task creation process as for very large\n    DAGs. Options can be set as string or using the constants defined in\n    the static class <code>airflow.utils.WeightRule</code>\n:type weight_rule: str\n:param queue: which queue to target when running this job. Not\n    all executors implement queue management, the CeleryExecutor\n    does support targeting specific queues.\n:type queue: str\n:param pool: the slot pool this task should run in, slot pools are a\n    way to limit concurrency for certain tasks\n:type pool: str\n:param pool_slots: the number of pool slots this task should use (&gt;= 1)\n    Values less than 1 are not allowed.\n:type pool_slots: int\n:param sla: time by which the job is expected to succeed. Note that\n    this represents the <code>timedelta</code> after the period is closed. For\n    example if you set an SLA of 1 hour, the scheduler would send an email\n    soon after 1:00AM on the <code>2016-01-02</code> if the <code>2016-01-01</code> instance\n    has not succeeded yet.\n    The scheduler pays special attention for jobs with an SLA and\n    sends alert\n    emails for SLA misses. SLA misses are also recorded in the database\n    for future reference. All tasks that share the same SLA time\n    get bundled in a single email, sent soon after that time. SLA\n    notification are sent once and only once for each task instance.\n:type sla: datetime.timedelta\n:param execution_timeout: max time allowed for the execution of\n    this task instance, if it goes beyond it will raise and fail.\n:type execution_timeout: datetime.timedelta\n:param on_failure_callback: a function to be called when a task instance\n    of this task fails. a context dictionary is passed as a single\n    parameter to this function. Context contains references to related\n    objects to the task instance and is documented under the macros\n    section of the API.\n:type on_failure_callback: TaskStateChangeCallback\n:param on_execute_callback: much like the <code>on_failure_callback</code> except\n    that it is executed right before the task is executed.\n:type on_execute_callback: TaskStateChangeCallback\n:param on_retry_callback: much like the <code>on_failure_callback</code> except\n    that it is executed when retries occur.\n:type on_retry_callback: TaskStateChangeCallback\n:param on_success_callback: much like the <code>on_failure_callback</code> except\n    that it is executed when the task succeeds.\n:type on_success_callback: TaskStateChangeCallback\n:param pre_execute: a function to be called immediately before task\n    execution, receiving a context dictionary; raising an exception will\n    prevent the task from being executed.</p>\n\n<pre><code>|experimental|\n</code></pre>\n\n<p>:type pre_execute: TaskPreExecuteHook\n:param post_execute: a function to be called immediately after task\n    execution, receiving a context dictionary and task result; raising an\n    exception will prevent the task from succeeding.</p>\n\n<pre><code>|experimental|\n</code></pre>\n\n<p>:type post_execute: TaskPostExecuteHook\n:param trigger_rule: defines the rule by which dependencies are applied\n    for the task to get triggered. Options are:\n    <code>{ all_success | all_failed | all_done | one_success |\n    one_failed | none_failed | none_failed_min_one_success | none_skipped | always}</code>\n    default is <code>all_success</code>. Options can be set as string or\n    using the constants defined in the static class\n    <code>airflow.utils.TriggerRule</code>\n:type trigger_rule: str\n:param resources: A map of resource parameter names (the argument names of the\n    Resources constructor) to their values.\n:type resources: dict\n:param run_as_user: unix username to impersonate while running the task\n:type run_as_user: str\n:param max_active_tis_per_dag: When set, a task will be able to limit the concurrent\n    runs across execution_dates.\n:type max_active_tis_per_dag: int\n:param executor_config: Additional task-level configuration parameters that are\n    interpreted by a specific executor. Parameters are namespaced by the name of\n    executor.</p>\n\n<pre><code>**Example**: to run this task in a specific docker container through\nthe KubernetesExecutor ::\n\n    MyOperator(...,\n        executor_config={\n            \"KubernetesExecutor\":\n                {\"image\": \"myCustomDockerImage\"}\n        }\n    )\n</code></pre>\n\n<p>:type executor_config: dict\n:param do_xcom_push: if True, an XCom is pushed containing the Operator's\n    result\n:type do_xcom_push: bool\n:param task_group: The TaskGroup to which the task should belong. This is typically provided when not\n    using a TaskGroup as a context manager.\n:type task_group: airflow.utils.task_group.TaskGroup\n:param doc: Add documentation or notes to your Task objects that is visible in\n    Task Instance details View in the Webserver\n:type doc: str\n:param doc_md: Add documentation (in Markdown format) or notes to your Task objects\n    that is visible in Task Instance details View in the Webserver\n:type doc_md: str\n:param doc_rst: Add documentation (in RST format) or notes to your Task objects\n    that is visible in Task Instance details View in the Webserver\n:type doc_rst: str\n:param doc_json: Add documentation (in JSON format) or notes to your Task objects\n    that is visible in Task Instance details View in the Webserver\n:type doc_json: str\n:param doc_yaml: Add documentation (in YAML format) or notes to your Task objects\n    that is visible in Task Instance details View in the Webserver\n:type doc_yaml: str</p>\n", "bases": "airflow.models.baseoperator.BaseOperator"}, {"fullname": "etna.operators.docker_operator_base.DockerOperatorBase.__init__", "modulename": "etna.operators.docker_operator_base", "qualname": "DockerOperatorBase.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    *args,\n    command: List[str],\n    swarm_shared_data: Union[List[etna.operators.docker_operator_base.SwarmSharedData], NoneType] = None,\n    serialize_last_output: Union[Callable[[bytes], Any], NoneType] = None,\n    docker_base_url='unix://var/run/docker.sock',\n    env: Mapping[str, str] = {},\n    **kwds\n)", "funcdef": "def"}, {"fullname": "etna.operators.docker_operator_base.DockerOperatorBase.serialize_last_output", "modulename": "etna.operators.docker_operator_base", "qualname": "DockerOperatorBase.serialize_last_output", "type": "variable", "doc": "<p></p>\n", "annotation": ": Union[Callable[[bytes], Any], NoneType]", "default_value": " = None"}, {"fullname": "etna.operators.docker_operator_base.DockerOperatorBase.template_fields", "modulename": "etna.operators.docker_operator_base", "qualname": "DockerOperatorBase.template_fields", "type": "variable", "doc": "<p></p>\n", "annotation": ": Iterable[str]", "default_value": " = ('env',)"}, {"fullname": "etna.operators.docker_operator_base.DockerOperatorBase.template_fields_renderers", "modulename": "etna.operators.docker_operator_base", "qualname": "DockerOperatorBase.template_fields_renderers", "type": "variable", "doc": "<p></p>\n", "annotation": ": Dict[str, str]", "default_value": " = {'env': 'json'}"}, {"fullname": "etna.operators.docker_operator_base.DockerOperatorBase.accepts", "modulename": "etna.operators.docker_operator_base", "qualname": "DockerOperatorBase.accepts", "type": "function", "doc": "<p></p>\n", "signature": "(self, *file_names: str)", "funcdef": "def"}, {"fullname": "etna.operators.docker_operator_base.DockerOperatorBase.pre_execute", "modulename": "etna.operators.docker_operator_base", "qualname": "DockerOperatorBase.pre_execute", "type": "function", "doc": "<p>This hook is triggered right before self.execute() is called.</p>\n", "signature": "(self, context) -> None", "funcdef": "def"}, {"fullname": "etna.operators.docker_operator_base.DockerOperatorBase.swarm_shared_data", "modulename": "etna.operators.docker_operator_base", "qualname": "DockerOperatorBase.swarm_shared_data", "type": "variable", "doc": "<p></p>\n"}, {"fullname": "etna.operators.docker_operator_base.DockerOperatorBase.set_input", "modulename": "etna.operators.docker_operator_base", "qualname": "DockerOperatorBase.set_input", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    key: str,\n    value: airflow.models.xcom_arg.XComArg\n) -> etna.operators.docker_operator_base.DockerOperatorBase", "funcdef": "def"}, {"fullname": "etna.operators.docker_operator_base.DockerOperatorBase.shared_data_labeling", "modulename": "etna.operators.docker_operator_base", "qualname": "DockerOperatorBase.shared_data_labeling", "type": "function", "doc": "<p></p>\n", "signature": "()", "funcdef": "def"}, {"fullname": "etna.operators.docker_operator_base.DockerOperatorBase.task_name", "modulename": "etna.operators.docker_operator_base", "qualname": "DockerOperatorBase.task_name", "type": "variable", "doc": "<p></p>\n", "annotation": ": str"}, {"fullname": "etna.operators.docker_operator_base.DockerOperatorBase.successful_states", "modulename": "etna.operators.docker_operator_base", "qualname": "DockerOperatorBase.successful_states", "type": "variable", "doc": "<p></p>\n", "annotation": ": Set[str]"}, {"fullname": "etna.operators.docker_operator_base.DockerOperatorBase.completed_states", "modulename": "etna.operators.docker_operator_base", "qualname": "DockerOperatorBase.completed_states", "type": "variable", "doc": "<p></p>\n", "annotation": ": Set[str]"}, {"fullname": "etna.operators.docker_operator_base.DockerOperatorBase.cleanup", "modulename": "etna.operators.docker_operator_base", "qualname": "DockerOperatorBase.cleanup", "type": "variable", "doc": "<p></p>\n"}, {"fullname": "etna.operators.docker_operator_base.DockerOperatorBase.on_kill", "modulename": "etna.operators.docker_operator_base", "qualname": "DockerOperatorBase.on_kill", "type": "function", "doc": "<p>Override this method to cleanup subprocesses when a task instance\ngets killed. Any use of the threading, subprocess or multiprocessing\nmodule within an operator needs to be cleaned up or it will leave\nghost processes behind.</p>\n", "signature": "(self) -> None", "funcdef": "def"}, {"fullname": "etna.operators.docker_operator_base.DockerOperatorBase.execute", "modulename": "etna.operators.docker_operator_base", "qualname": "DockerOperatorBase.execute", "type": "function", "doc": "<p>This is the main method to derive when creating an operator.\nContext is the same dictionary used as when rendering jinja templates.</p>\n\n<p>Refer to get_template_context for more context.</p>\n", "signature": "(self, context: Any)", "funcdef": "def"}, {"fullname": "etna.operators.docker_operator_base.write_logs_and_yield_last", "modulename": "etna.operators.docker_operator_base", "qualname": "write_logs_and_yield_last", "type": "function", "doc": "<p></p>\n", "signature": "(\n    next_batch_since: Callable[[int], bytes],\n    log: logging.Logger\n) -> Iterator[bytes]", "funcdef": "def"}, {"fullname": "etna.operators.rollup_xcom_operator", "modulename": "etna.operators.rollup_xcom_operator", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.operators.rollup_xcom_operator.RollupState", "modulename": "etna.operators.rollup_xcom_operator", "qualname": "RollupState", "type": "class", "doc": "<p></p>\n", "bases": "etna.metrics.rollup_metrics.Rollup"}, {"fullname": "etna.operators.rollup_xcom_operator.RollupState.__init__", "modulename": "etna.operators.rollup_xcom_operator", "qualname": "RollupState.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    last_execution_date: datetime.datetime,\n    result: etna.metrics.rollup_metrics.Rollup\n)", "funcdef": "def"}, {"fullname": "etna.operators.rollup_xcom_operator.RollupState.measure", "modulename": "etna.operators.rollup_xcom_operator", "qualname": "RollupState.measure", "type": "function", "doc": "<p></p>\n", "signature": "(self) -> Iterable[Tuple[Dict[str, str], int]]", "funcdef": "def"}, {"fullname": "etna.operators.rollup_xcom_operator.rollup", "modulename": "etna.operators.rollup_xcom_operator", "qualname": "rollup", "type": "class", "doc": "<p></p>\n"}, {"fullname": "etna.operators.rollup_xcom_operator.rollup.__init__", "modulename": "etna.operators.rollup_xcom_operator", "qualname": "rollup.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    concat: Callable[[etna.metrics.rollup_metrics.Rollup, etna.metrics.rollup_metrics.Rollup], etna.metrics.rollup_metrics.Rollup]\n)", "funcdef": "def"}, {"fullname": "etna.operators.rollup_xcom_operator.RollupXcomOperator", "modulename": "etna.operators.rollup_xcom_operator", "qualname": "RollupXcomOperator", "type": "class", "doc": "<p>Abstract base class for all operators. Since operators create objects that\nbecome nodes in the dag, BaseOperator contains many recursive methods for\ndag crawling behavior. To derive this class, you are expected to override\nthe constructor as well as the 'execute' method.</p>\n\n<p>Operators derived from this class should perform or trigger certain tasks\nsynchronously (wait for completion). Example of operators could be an\noperator that runs a Pig job (PigOperator), a sensor operator that\nwaits for a partition to land in Hive (HiveSensorOperator), or one that\nmoves data from Hive to MySQL (Hive2MySqlOperator). Instances of these\noperators (tasks) target specific operations, running specific scripts,\nfunctions or data transfers.</p>\n\n<p>This class is abstract and shouldn't be instantiated. Instantiating a\nclass derived from this one results in the creation of a task object,\nwhich ultimately becomes a node in DAG objects. Task dependencies should\nbe set by using the set_upstream and/or set_downstream methods.</p>\n\n<p>:param task_id: a unique, meaningful id for the task\n:type task_id: str\n:param owner: the owner of the task. Using a meaningful description\n    (e.g. user/person/team/role name) to clarify ownership is recommended.\n:type owner: str\n:param email: the 'to' email address(es) used in email alerts. This can be a\n    single email or multiple ones. Multiple addresses can be specified as a\n    comma or semi-colon separated string or by passing a list of strings.\n:type email: str or list[str]\n:param email_on_retry: Indicates whether email alerts should be sent when a\n    task is retried\n:type email_on_retry: bool\n:param email_on_failure: Indicates whether email alerts should be sent when\n    a task failed\n:type email_on_failure: bool\n:param retries: the number of retries that should be performed before\n    failing the task\n:type retries: int\n:param retry_delay: delay between retries\n:type retry_delay: datetime.timedelta\n:param retry_exponential_backoff: allow progressive longer waits between\n    retries by using exponential backoff algorithm on retry delay (delay\n    will be converted into seconds)\n:type retry_exponential_backoff: bool\n:param max_retry_delay: maximum delay interval between retries\n:type max_retry_delay: datetime.timedelta\n:param start_date: The <code>start_date</code> for the task, determines\n    the <code>execution_date</code> for the first task instance. The best practice\n    is to have the start_date rounded\n    to your DAG's <code>schedule_interval</code>. Daily jobs have their start_date\n    some day at 00:00:00, hourly jobs have their start_date at 00:00\n    of a specific hour. Note that Airflow simply looks at the latest\n    <code>execution_date</code> and adds the <code>schedule_interval</code> to determine\n    the next <code>execution_date</code>. It is also very important\n    to note that different tasks' dependencies\n    need to line up in time. If task A depends on task B and their\n    start_date are offset in a way that their execution_date don't line\n    up, A's dependencies will never be met. If you are looking to delay\n    a task, for example running a daily task at 2AM, look into the\n    <code>TimeSensor</code> and <code>TimeDeltaSensor</code>. We advise against using\n    dynamic <code>start_date</code> and recommend using fixed ones. Read the\n    FAQ entry about start_date for more information.\n:type start_date: datetime.datetime\n:param end_date: if specified, the scheduler won't go beyond this date\n:type end_date: datetime.datetime\n:param depends_on_past: when set to true, task instances will run\n    sequentially and only if the previous instance has succeeded or has been skipped.\n    The task instance for the start_date is allowed to run.\n:type depends_on_past: bool\n:param wait_for_downstream: when set to true, an instance of task\n    X will wait for tasks immediately downstream of the previous instance\n    of task X to finish successfully or be skipped before it runs. This is useful if the\n    different instances of a task X alter the same asset, and this asset\n    is used by tasks downstream of task X. Note that depends_on_past\n    is forced to True wherever wait_for_downstream is used. Also note that\n    only tasks <em>immediately</em> downstream of the previous task instance are waited\n    for; the statuses of any tasks further downstream are ignored.\n:type wait_for_downstream: bool\n:param dag: a reference to the dag the task is attached to (if any)\n:type dag: airflow.models.DAG\n:param priority_weight: priority weight of this task against other task.\n    This allows the executor to trigger higher priority tasks before\n    others when things get backed up. Set priority_weight as a higher\n    number for more important tasks.\n:type priority_weight: int\n:param weight_rule: weighting method used for the effective total\n    priority weight of the task. Options are:\n    <code>{ downstream | upstream | absolute }</code> default is <code>downstream</code>\n    When set to <code>downstream</code> the effective weight of the task is the\n    aggregate sum of all downstream descendants. As a result, upstream\n    tasks will have higher weight and will be scheduled more aggressively\n    when using positive weight values. This is useful when you have\n    multiple dag run instances and desire to have all upstream tasks to\n    complete for all runs before each dag can continue processing\n    downstream tasks. When set to <code>upstream</code> the effective weight is the\n    aggregate sum of all upstream ancestors. This is the opposite where\n    downstream tasks have higher weight and will be scheduled more\n    aggressively when using positive weight values. This is useful when you\n    have multiple dag run instances and prefer to have each dag complete\n    before starting upstream tasks of other dags.  When set to\n    <code>absolute</code>, the effective weight is the exact <code>priority_weight</code>\n    specified without additional weighting. You may want to do this when\n    you know exactly what priority weight each task should have.\n    Additionally, when set to <code>absolute</code>, there is bonus effect of\n    significantly speeding up the task creation process as for very large\n    DAGs. Options can be set as string or using the constants defined in\n    the static class <code>airflow.utils.WeightRule</code>\n:type weight_rule: str\n:param queue: which queue to target when running this job. Not\n    all executors implement queue management, the CeleryExecutor\n    does support targeting specific queues.\n:type queue: str\n:param pool: the slot pool this task should run in, slot pools are a\n    way to limit concurrency for certain tasks\n:type pool: str\n:param pool_slots: the number of pool slots this task should use (&gt;= 1)\n    Values less than 1 are not allowed.\n:type pool_slots: int\n:param sla: time by which the job is expected to succeed. Note that\n    this represents the <code>timedelta</code> after the period is closed. For\n    example if you set an SLA of 1 hour, the scheduler would send an email\n    soon after 1:00AM on the <code>2016-01-02</code> if the <code>2016-01-01</code> instance\n    has not succeeded yet.\n    The scheduler pays special attention for jobs with an SLA and\n    sends alert\n    emails for SLA misses. SLA misses are also recorded in the database\n    for future reference. All tasks that share the same SLA time\n    get bundled in a single email, sent soon after that time. SLA\n    notification are sent once and only once for each task instance.\n:type sla: datetime.timedelta\n:param execution_timeout: max time allowed for the execution of\n    this task instance, if it goes beyond it will raise and fail.\n:type execution_timeout: datetime.timedelta\n:param on_failure_callback: a function to be called when a task instance\n    of this task fails. a context dictionary is passed as a single\n    parameter to this function. Context contains references to related\n    objects to the task instance and is documented under the macros\n    section of the API.\n:type on_failure_callback: TaskStateChangeCallback\n:param on_execute_callback: much like the <code>on_failure_callback</code> except\n    that it is executed right before the task is executed.\n:type on_execute_callback: TaskStateChangeCallback\n:param on_retry_callback: much like the <code>on_failure_callback</code> except\n    that it is executed when retries occur.\n:type on_retry_callback: TaskStateChangeCallback\n:param on_success_callback: much like the <code>on_failure_callback</code> except\n    that it is executed when the task succeeds.\n:type on_success_callback: TaskStateChangeCallback\n:param pre_execute: a function to be called immediately before task\n    execution, receiving a context dictionary; raising an exception will\n    prevent the task from being executed.</p>\n\n<pre><code>|experimental|\n</code></pre>\n\n<p>:type pre_execute: TaskPreExecuteHook\n:param post_execute: a function to be called immediately after task\n    execution, receiving a context dictionary and task result; raising an\n    exception will prevent the task from succeeding.</p>\n\n<pre><code>|experimental|\n</code></pre>\n\n<p>:type post_execute: TaskPostExecuteHook\n:param trigger_rule: defines the rule by which dependencies are applied\n    for the task to get triggered. Options are:\n    <code>{ all_success | all_failed | all_done | one_success |\n    one_failed | none_failed | none_failed_min_one_success | none_skipped | always}</code>\n    default is <code>all_success</code>. Options can be set as string or\n    using the constants defined in the static class\n    <code>airflow.utils.TriggerRule</code>\n:type trigger_rule: str\n:param resources: A map of resource parameter names (the argument names of the\n    Resources constructor) to their values.\n:type resources: dict\n:param run_as_user: unix username to impersonate while running the task\n:type run_as_user: str\n:param max_active_tis_per_dag: When set, a task will be able to limit the concurrent\n    runs across execution_dates.\n:type max_active_tis_per_dag: int\n:param executor_config: Additional task-level configuration parameters that are\n    interpreted by a specific executor. Parameters are namespaced by the name of\n    executor.</p>\n\n<pre><code>**Example**: to run this task in a specific docker container through\nthe KubernetesExecutor ::\n\n    MyOperator(...,\n        executor_config={\n            \"KubernetesExecutor\":\n                {\"image\": \"myCustomDockerImage\"}\n        }\n    )\n</code></pre>\n\n<p>:type executor_config: dict\n:param do_xcom_push: if True, an XCom is pushed containing the Operator's\n    result\n:type do_xcom_push: bool\n:param task_group: The TaskGroup to which the task should belong. This is typically provided when not\n    using a TaskGroup as a context manager.\n:type task_group: airflow.utils.task_group.TaskGroup\n:param doc: Add documentation or notes to your Task objects that is visible in\n    Task Instance details View in the Webserver\n:type doc: str\n:param doc_md: Add documentation (in Markdown format) or notes to your Task objects\n    that is visible in Task Instance details View in the Webserver\n:type doc_md: str\n:param doc_rst: Add documentation (in RST format) or notes to your Task objects\n    that is visible in Task Instance details View in the Webserver\n:type doc_rst: str\n:param doc_json: Add documentation (in JSON format) or notes to your Task objects\n    that is visible in Task Instance details View in the Webserver\n:type doc_json: str\n:param doc_yaml: Add documentation (in YAML format) or notes to your Task objects\n    that is visible in Task Instance details View in the Webserver\n:type doc_yaml: str</p>\n", "bases": "airflow.models.baseoperator.BaseOperator"}, {"fullname": "etna.operators.rollup_xcom_operator.RollupXcomOperator.__init__", "modulename": "etna.operators.rollup_xcom_operator", "qualname": "RollupXcomOperator.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    concat: Callable[[etna.metrics.rollup_metrics.Rollup, etna.metrics.rollup_metrics.Rollup], etna.metrics.rollup_metrics.Rollup],\n    arg: Union[airflow.models.xcom_arg.XComArg, NoneType] = None,\n    target_dag_id: Union[str, NoneType] = None,\n    target_task_id: Union[str, NoneType] = None,\n    task_id: Union[str, NoneType] = None,\n    **kwargs\n)", "funcdef": "def"}, {"fullname": "etna.operators.rollup_xcom_operator.RollupXcomOperator.execute", "modulename": "etna.operators.rollup_xcom_operator", "qualname": "RollupXcomOperator.execute", "type": "function", "doc": "<p>This is the main method to derive when creating an operator.\nContext is the same dictionary used as when rendering jinja templates.</p>\n\n<p>Refer to get_template_context for more context.</p>\n", "signature": "(self, context: Any, session: sqlalchemy.orm.session.Session = None)", "funcdef": "def"}, {"fullname": "etna.operators.swarm_operator", "modulename": "etna.operators.swarm_operator", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.operators.swarm_operator.join_labels", "modulename": "etna.operators.swarm_operator", "qualname": "join_labels", "type": "function", "doc": "<p></p>\n", "signature": "(labels: Dict[str, str]) -> List[str]", "funcdef": "def"}, {"fullname": "etna.operators.swarm_operator.SwarmServiceDefinition", "modulename": "etna.operators.swarm_operator", "qualname": "SwarmServiceDefinition", "type": "class", "doc": "<p>SwarmServiceDefinition(image: str, command: List[str], mounts: Union[List[docker.types.services.Mount], NoneType], env: List[str], user: Union[str, NoneType], tty: bool, configs: List[docker.types.services.ConfigReference], secrets: List[docker.types.services.SecretReference], resources: Union[docker.types.services.Resources, NoneType], networks: List[docker.types.services.NetworkAttachmentConfig], placement: docker.types.services.Placement)</p>\n"}, {"fullname": "etna.operators.swarm_operator.SwarmServiceDefinition.__init__", "modulename": "etna.operators.swarm_operator", "qualname": "SwarmServiceDefinition.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    image: str,\n    command: List[str],\n    mounts: Union[List[docker.types.services.Mount], NoneType],\n    env: List[str],\n    user: Union[str, NoneType],\n    tty: bool,\n    configs: List[docker.types.services.ConfigReference],\n    secrets: List[docker.types.services.SecretReference],\n    resources: Union[docker.types.services.Resources, NoneType],\n    networks: List[docker.types.services.NetworkAttachmentConfig],\n    placement: docker.types.services.Placement\n)", "funcdef": "def"}, {"fullname": "etna.operators.swarm_operator.find_service", "modulename": "etna.operators.swarm_operator", "qualname": "find_service", "type": "function", "doc": "<p></p>\n", "signature": "(cli: docker.api.client.APIClient, service_name: str) -> Dict", "funcdef": "def"}, {"fullname": "etna.operators.swarm_operator.find_local_network_ids", "modulename": "etna.operators.swarm_operator", "qualname": "find_local_network_ids", "type": "function", "doc": "<p></p>\n", "signature": "(cli: docker.api.client.APIClient, data: Dict) -> Set[str]", "funcdef": "def"}, {"fullname": "etna.operators.swarm_operator.create_service_definition", "modulename": "etna.operators.swarm_operator", "qualname": "create_service_definition", "type": "function", "doc": "<p></p>\n", "signature": "(\n    data: Dict,\n    local_network_ids: Union[Set[str], NoneType] = None\n) -> etna.operators.swarm_operator.SwarmServiceDefinition", "funcdef": "def"}, {"fullname": "etna.operators.swarm_operator.create_service_from_definition", "modulename": "etna.operators.swarm_operator", "qualname": "create_service_from_definition", "type": "function", "doc": "<p></p>\n", "signature": "(\n    cli: docker.api.client.APIClient,\n    service_definition: etna.operators.swarm_operator.SwarmServiceDefinition,\n    service_name: str,\n    extr_env: Mapping[str, str],\n    labeling: Dict[str, str]\n)", "funcdef": "def"}, {"fullname": "etna.operators.swarm_operator.swarm_cleanup", "modulename": "etna.operators.swarm_operator", "qualname": "swarm_cleanup", "type": "function", "doc": "<p></p>\n", "signature": "(cli: docker.api.client.APIClient)", "funcdef": "def"}, {"fullname": "etna.operators.swarm_operator.DockerSwarmOperator", "modulename": "etna.operators.swarm_operator", "qualname": "DockerSwarmOperator", "type": "class", "doc": "<p>Abstract base class for all operators. Since operators create objects that\nbecome nodes in the dag, BaseOperator contains many recursive methods for\ndag crawling behavior. To derive this class, you are expected to override\nthe constructor as well as the 'execute' method.</p>\n\n<p>Operators derived from this class should perform or trigger certain tasks\nsynchronously (wait for completion). Example of operators could be an\noperator that runs a Pig job (PigOperator), a sensor operator that\nwaits for a partition to land in Hive (HiveSensorOperator), or one that\nmoves data from Hive to MySQL (Hive2MySqlOperator). Instances of these\noperators (tasks) target specific operations, running specific scripts,\nfunctions or data transfers.</p>\n\n<p>This class is abstract and shouldn't be instantiated. Instantiating a\nclass derived from this one results in the creation of a task object,\nwhich ultimately becomes a node in DAG objects. Task dependencies should\nbe set by using the set_upstream and/or set_downstream methods.</p>\n\n<p>:param task_id: a unique, meaningful id for the task\n:type task_id: str\n:param owner: the owner of the task. Using a meaningful description\n    (e.g. user/person/team/role name) to clarify ownership is recommended.\n:type owner: str\n:param email: the 'to' email address(es) used in email alerts. This can be a\n    single email or multiple ones. Multiple addresses can be specified as a\n    comma or semi-colon separated string or by passing a list of strings.\n:type email: str or list[str]\n:param email_on_retry: Indicates whether email alerts should be sent when a\n    task is retried\n:type email_on_retry: bool\n:param email_on_failure: Indicates whether email alerts should be sent when\n    a task failed\n:type email_on_failure: bool\n:param retries: the number of retries that should be performed before\n    failing the task\n:type retries: int\n:param retry_delay: delay between retries\n:type retry_delay: datetime.timedelta\n:param retry_exponential_backoff: allow progressive longer waits between\n    retries by using exponential backoff algorithm on retry delay (delay\n    will be converted into seconds)\n:type retry_exponential_backoff: bool\n:param max_retry_delay: maximum delay interval between retries\n:type max_retry_delay: datetime.timedelta\n:param start_date: The <code>start_date</code> for the task, determines\n    the <code>execution_date</code> for the first task instance. The best practice\n    is to have the start_date rounded\n    to your DAG's <code>schedule_interval</code>. Daily jobs have their start_date\n    some day at 00:00:00, hourly jobs have their start_date at 00:00\n    of a specific hour. Note that Airflow simply looks at the latest\n    <code>execution_date</code> and adds the <code>schedule_interval</code> to determine\n    the next <code>execution_date</code>. It is also very important\n    to note that different tasks' dependencies\n    need to line up in time. If task A depends on task B and their\n    start_date are offset in a way that their execution_date don't line\n    up, A's dependencies will never be met. If you are looking to delay\n    a task, for example running a daily task at 2AM, look into the\n    <code>TimeSensor</code> and <code>TimeDeltaSensor</code>. We advise against using\n    dynamic <code>start_date</code> and recommend using fixed ones. Read the\n    FAQ entry about start_date for more information.\n:type start_date: datetime.datetime\n:param end_date: if specified, the scheduler won't go beyond this date\n:type end_date: datetime.datetime\n:param depends_on_past: when set to true, task instances will run\n    sequentially and only if the previous instance has succeeded or has been skipped.\n    The task instance for the start_date is allowed to run.\n:type depends_on_past: bool\n:param wait_for_downstream: when set to true, an instance of task\n    X will wait for tasks immediately downstream of the previous instance\n    of task X to finish successfully or be skipped before it runs. This is useful if the\n    different instances of a task X alter the same asset, and this asset\n    is used by tasks downstream of task X. Note that depends_on_past\n    is forced to True wherever wait_for_downstream is used. Also note that\n    only tasks <em>immediately</em> downstream of the previous task instance are waited\n    for; the statuses of any tasks further downstream are ignored.\n:type wait_for_downstream: bool\n:param dag: a reference to the dag the task is attached to (if any)\n:type dag: airflow.models.DAG\n:param priority_weight: priority weight of this task against other task.\n    This allows the executor to trigger higher priority tasks before\n    others when things get backed up. Set priority_weight as a higher\n    number for more important tasks.\n:type priority_weight: int\n:param weight_rule: weighting method used for the effective total\n    priority weight of the task. Options are:\n    <code>{ downstream | upstream | absolute }</code> default is <code>downstream</code>\n    When set to <code>downstream</code> the effective weight of the task is the\n    aggregate sum of all downstream descendants. As a result, upstream\n    tasks will have higher weight and will be scheduled more aggressively\n    when using positive weight values. This is useful when you have\n    multiple dag run instances and desire to have all upstream tasks to\n    complete for all runs before each dag can continue processing\n    downstream tasks. When set to <code>upstream</code> the effective weight is the\n    aggregate sum of all upstream ancestors. This is the opposite where\n    downstream tasks have higher weight and will be scheduled more\n    aggressively when using positive weight values. This is useful when you\n    have multiple dag run instances and prefer to have each dag complete\n    before starting upstream tasks of other dags.  When set to\n    <code>absolute</code>, the effective weight is the exact <code>priority_weight</code>\n    specified without additional weighting. You may want to do this when\n    you know exactly what priority weight each task should have.\n    Additionally, when set to <code>absolute</code>, there is bonus effect of\n    significantly speeding up the task creation process as for very large\n    DAGs. Options can be set as string or using the constants defined in\n    the static class <code>airflow.utils.WeightRule</code>\n:type weight_rule: str\n:param queue: which queue to target when running this job. Not\n    all executors implement queue management, the CeleryExecutor\n    does support targeting specific queues.\n:type queue: str\n:param pool: the slot pool this task should run in, slot pools are a\n    way to limit concurrency for certain tasks\n:type pool: str\n:param pool_slots: the number of pool slots this task should use (&gt;= 1)\n    Values less than 1 are not allowed.\n:type pool_slots: int\n:param sla: time by which the job is expected to succeed. Note that\n    this represents the <code>timedelta</code> after the period is closed. For\n    example if you set an SLA of 1 hour, the scheduler would send an email\n    soon after 1:00AM on the <code>2016-01-02</code> if the <code>2016-01-01</code> instance\n    has not succeeded yet.\n    The scheduler pays special attention for jobs with an SLA and\n    sends alert\n    emails for SLA misses. SLA misses are also recorded in the database\n    for future reference. All tasks that share the same SLA time\n    get bundled in a single email, sent soon after that time. SLA\n    notification are sent once and only once for each task instance.\n:type sla: datetime.timedelta\n:param execution_timeout: max time allowed for the execution of\n    this task instance, if it goes beyond it will raise and fail.\n:type execution_timeout: datetime.timedelta\n:param on_failure_callback: a function to be called when a task instance\n    of this task fails. a context dictionary is passed as a single\n    parameter to this function. Context contains references to related\n    objects to the task instance and is documented under the macros\n    section of the API.\n:type on_failure_callback: TaskStateChangeCallback\n:param on_execute_callback: much like the <code>on_failure_callback</code> except\n    that it is executed right before the task is executed.\n:type on_execute_callback: TaskStateChangeCallback\n:param on_retry_callback: much like the <code>on_failure_callback</code> except\n    that it is executed when retries occur.\n:type on_retry_callback: TaskStateChangeCallback\n:param on_success_callback: much like the <code>on_failure_callback</code> except\n    that it is executed when the task succeeds.\n:type on_success_callback: TaskStateChangeCallback\n:param pre_execute: a function to be called immediately before task\n    execution, receiving a context dictionary; raising an exception will\n    prevent the task from being executed.</p>\n\n<pre><code>|experimental|\n</code></pre>\n\n<p>:type pre_execute: TaskPreExecuteHook\n:param post_execute: a function to be called immediately after task\n    execution, receiving a context dictionary and task result; raising an\n    exception will prevent the task from succeeding.</p>\n\n<pre><code>|experimental|\n</code></pre>\n\n<p>:type post_execute: TaskPostExecuteHook\n:param trigger_rule: defines the rule by which dependencies are applied\n    for the task to get triggered. Options are:\n    <code>{ all_success | all_failed | all_done | one_success |\n    one_failed | none_failed | none_failed_min_one_success | none_skipped | always}</code>\n    default is <code>all_success</code>. Options can be set as string or\n    using the constants defined in the static class\n    <code>airflow.utils.TriggerRule</code>\n:type trigger_rule: str\n:param resources: A map of resource parameter names (the argument names of the\n    Resources constructor) to their values.\n:type resources: dict\n:param run_as_user: unix username to impersonate while running the task\n:type run_as_user: str\n:param max_active_tis_per_dag: When set, a task will be able to limit the concurrent\n    runs across execution_dates.\n:type max_active_tis_per_dag: int\n:param executor_config: Additional task-level configuration parameters that are\n    interpreted by a specific executor. Parameters are namespaced by the name of\n    executor.</p>\n\n<pre><code>**Example**: to run this task in a specific docker container through\nthe KubernetesExecutor ::\n\n    MyOperator(...,\n        executor_config={\n            \"KubernetesExecutor\":\n                {\"image\": \"myCustomDockerImage\"}\n        }\n    )\n</code></pre>\n\n<p>:type executor_config: dict\n:param do_xcom_push: if True, an XCom is pushed containing the Operator's\n    result\n:type do_xcom_push: bool\n:param task_group: The TaskGroup to which the task should belong. This is typically provided when not\n    using a TaskGroup as a context manager.\n:type task_group: airflow.utils.task_group.TaskGroup\n:param doc: Add documentation or notes to your Task objects that is visible in\n    Task Instance details View in the Webserver\n:type doc: str\n:param doc_md: Add documentation (in Markdown format) or notes to your Task objects\n    that is visible in Task Instance details View in the Webserver\n:type doc_md: str\n:param doc_rst: Add documentation (in RST format) or notes to your Task objects\n    that is visible in Task Instance details View in the Webserver\n:type doc_rst: str\n:param doc_json: Add documentation (in JSON format) or notes to your Task objects\n    that is visible in Task Instance details View in the Webserver\n:type doc_json: str\n:param doc_yaml: Add documentation (in YAML format) or notes to your Task objects\n    that is visible in Task Instance details View in the Webserver\n:type doc_yaml: str</p>\n", "bases": "etna.operators.docker_operator_base.DockerOperatorBase"}, {"fullname": "etna.operators.swarm_operator.DockerSwarmOperator.__init__", "modulename": "etna.operators.swarm_operator", "qualname": "DockerSwarmOperator.__init__", "type": "function", "doc": "<p></p>\n", "signature": "(\n    self,\n    *args,\n    source_service: str,\n    include_external_networks: Union[bool, NoneType] = False,\n    **kwds\n)", "funcdef": "def"}, {"fullname": "etna.operators.swarm_operator.DockerSwarmOperator.successful_states", "modulename": "etna.operators.swarm_operator", "qualname": "DockerSwarmOperator.successful_states", "type": "variable", "doc": "<p></p>\n", "annotation": ": Set[str]"}, {"fullname": "etna.operators.swarm_operator.DockerSwarmOperator.completed_states", "modulename": "etna.operators.swarm_operator", "qualname": "DockerSwarmOperator.completed_states", "type": "variable", "doc": "<p></p>\n", "annotation": ": Set[str]"}, {"fullname": "etna.operators.swarm_operator.DockerSwarmOperator.shared_data_labeling", "modulename": "etna.operators.swarm_operator", "qualname": "DockerSwarmOperator.shared_data_labeling", "type": "function", "doc": "<p></p>\n", "signature": "()", "funcdef": "def"}, {"fullname": "etna.operators.swarm_operator.DockerSwarmOperator.service_labeling", "modulename": "etna.operators.swarm_operator", "qualname": "DockerSwarmOperator.service_labeling", "type": "function", "doc": "<p></p>\n", "signature": "()", "funcdef": "def"}, {"fullname": "etna.operators.swarm_operator.DockerSwarmOperator.cleanup", "modulename": "etna.operators.swarm_operator", "qualname": "DockerSwarmOperator.cleanup", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "etna.operators.swarm_operator.hacky_task_logs", "modulename": "etna.operators.swarm_operator", "qualname": "hacky_task_logs", "type": "function", "doc": "<p></p>\n", "signature": "(\n    cli: docker.api.client.APIClient,\n    task_id: str,\n    since: int,\n    is_tty: bool\n) -> bytes", "funcdef": "def"}, {"fullname": "etna.xcom", "modulename": "etna.xcom", "type": "module", "doc": "<p></p>\n"}, {"fullname": "etna.xcom.etna_xcom", "modulename": "etna.xcom.etna_xcom", "type": "module", "doc": "<p>Improves upon the base airflow xcom with</p>\n\n<ol>\n<li>lzma compression of xcom values</li>\n<li>Ability to subclass EtnaXComValue to support\na. custom string summary in UI\nb. custom deferred value expansion via execute</li>\n</ol>\n"}, {"fullname": "etna.xcom.etna_xcom.EtnaXComValue", "modulename": "etna.xcom.etna_xcom", "qualname": "EtnaXComValue", "type": "class", "doc": "<p></p>\n"}, {"fullname": "etna.xcom.etna_xcom.EtnaXComValue.__init__", "modulename": "etna.xcom.etna_xcom", "qualname": "EtnaXComValue.__init__", "type": "function", "doc": "<p></p>\n", "signature": "()", "funcdef": "def"}, {"fullname": "etna.xcom.etna_xcom.EtnaXComValue.execute", "modulename": "etna.xcom.etna_xcom", "qualname": "EtnaXComValue.execute", "type": "function", "doc": "<p></p>\n", "signature": "(self)", "funcdef": "def"}, {"fullname": "etna.xcom.etna_xcom.pickled", "modulename": "etna.xcom.etna_xcom", "qualname": "pickled", "type": "function", "doc": "<p></p>\n", "signature": "(v: ~T) -> ~T", "funcdef": "def"}, {"fullname": "etna.xcom.etna_xcom.EtnaXCom", "modulename": "etna.xcom.etna_xcom", "qualname": "EtnaXCom", "type": "class", "doc": "<p>Base class for XCom objects.</p>\n", "bases": "airflow.models.xcom.BaseXCom"}, {"fullname": "etna.xcom.etna_xcom.EtnaXCom.__init__", "modulename": "etna.xcom.etna_xcom", "qualname": "EtnaXCom.__init__", "type": "function", "doc": "<p>A simple constructor that allows initialization from kwargs.</p>\n\n<p>Sets attributes on the constructed instance using the names and\nvalues in <code>kwargs</code>.</p>\n\n<p>Only keys that are present as\nattributes of the instance's class are allowed. These could be,\nfor example, any mapped columns or relationships.</p>\n", "signature": "(self, **kwargs)", "funcdef": "def"}, {"fullname": "etna.xcom.etna_xcom.EtnaXCom.serialize_value", "modulename": "etna.xcom.etna_xcom", "qualname": "EtnaXCom.serialize_value", "type": "function", "doc": "<p>Serialize Xcom value to str or pickled object</p>\n", "signature": "(value: Any)", "funcdef": "def"}, {"fullname": "etna.xcom.etna_xcom.EtnaXCom.deserialize_value", "modulename": "etna.xcom.etna_xcom", "qualname": "EtnaXCom.deserialize_value", "type": "function", "doc": "<p>Deserialize XCom value from str or pickle object</p>\n", "signature": "(result: etna.xcom.etna_xcom.EtnaXCom) -> Any", "funcdef": "def"}, {"fullname": "etna.xcom.etna_xcom.EtnaXCom.orm_deserialize_value", "modulename": "etna.xcom.etna_xcom", "qualname": "EtnaXCom.orm_deserialize_value", "type": "function", "doc": "<p>Deserialize method which is used to reconstruct ORM XCom object.</p>\n\n<p>This method should be overridden in custom XCom backends to avoid\nunnecessary request or other resource consuming operations when\ncreating XCom orm model. This is used when viewing XCom listing\nin the webserver, for example.</p>\n", "signature": "(self) -> Any", "funcdef": "def"}, {"fullname": "etna.xcom.etna_xcom.EtnaXCom.key", "modulename": "etna.xcom.etna_xcom", "qualname": "EtnaXCom.key", "type": "variable", "doc": "<p></p>\n"}, {"fullname": "etna.xcom.etna_xcom.EtnaXCom.value", "modulename": "etna.xcom.etna_xcom", "qualname": "EtnaXCom.value", "type": "variable", "doc": "<p></p>\n"}, {"fullname": "etna.xcom.etna_xcom.EtnaXCom.timestamp", "modulename": "etna.xcom.etna_xcom", "qualname": "EtnaXCom.timestamp", "type": "variable", "doc": "<p></p>\n"}, {"fullname": "etna.xcom.etna_xcom.EtnaXCom.execution_date", "modulename": "etna.xcom.etna_xcom", "qualname": "EtnaXCom.execution_date", "type": "variable", "doc": "<p></p>\n"}, {"fullname": "etna.xcom.etna_xcom.EtnaXCom.task_id", "modulename": "etna.xcom.etna_xcom", "qualname": "EtnaXCom.task_id", "type": "variable", "doc": "<p></p>\n"}, {"fullname": "etna.xcom.etna_xcom.EtnaXCom.dag_id", "modulename": "etna.xcom.etna_xcom", "qualname": "EtnaXCom.dag_id", "type": "variable", "doc": "<p></p>\n"}, {"fullname": "etna.xcom.etna_xcom.EtnaXCom.dag_run", "modulename": "etna.xcom.etna_xcom", "qualname": "EtnaXCom.dag_run", "type": "variable", "doc": "<p></p>\n"}];

    // mirrored in build-search-index.js (part 1)
    // Also split on html tags. this is a cheap heuristic, but good enough.
    elasticlunr.tokenizer.setSeperator(/[\s\-.;&_'"=,()]+|<[^>]*>/);

    let searchIndex;
    if (docs._isPrebuiltIndex) {
        console.info("using precompiled search index");
        searchIndex = elasticlunr.Index.load(docs);
    } else {
        console.time("building search index");
        // mirrored in build-search-index.js (part 2)
        searchIndex = elasticlunr(function () {
            this.pipeline.remove(elasticlunr.stemmer);
            this.pipeline.remove(elasticlunr.stopWordFilter);
            this.addField("qualname");
            this.addField("fullname");
            this.addField("annotation");
            this.addField("default_value");
            this.addField("signature");
            this.addField("bases");
            this.addField("doc");
            this.setRef("fullname");
        });
        for (let doc of docs) {
            searchIndex.addDoc(doc);
        }
        console.timeEnd("building search index");
    }

    return (term) => searchIndex.search(term, {
        fields: {
            qualname: {boost: 4},
            fullname: {boost: 2},
            annotation: {boost: 2},
            default_value: {boost: 2},
            signature: {boost: 2},
            bases: {boost: 2},
            doc: {boost: 1},
        },
        expand: true
    });
})();